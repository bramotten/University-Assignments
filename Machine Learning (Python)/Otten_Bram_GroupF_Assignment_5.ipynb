{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bram Otten\n",
    "### UvA ID: 10992456\n",
    "### Group F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data(datafile):\n",
    "    return np.loadtxt(open(datafile, 'r'),\n",
    "                      dtype='str',\n",
    "                      delimiter=',')\n",
    "\n",
    "\n",
    "def count_missing(data):\n",
    "    n = 0\n",
    "    for row in data:\n",
    "        for value in row:\n",
    "            if value == '?':\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "print(\"Loading Cleveland data.\")\n",
    "cdata = load_data('data5/processed.cleveland.data')\n",
    "print(\"Size:\", len(cdata), \"x\", len(cdata[0]))\n",
    "print(\"First row:\", cdata[0, :])\n",
    "nmc = count_missing(cdata)\n",
    "print(nmc, \"missing values.\")\n",
    "\n",
    "print()\n",
    "print(\"Loading Hungarian data.\")\n",
    "hdata = load_data('data5/processed.hungarian.data')\n",
    "print(\"Size:\", len(hdata), \"x\", len(hdata[0]))\n",
    "print(\"First row:\", hdata[0, :])\n",
    "hmc = count_missing(hdata)\n",
    "print(hmc, \"missing values.\")\n",
    "\n",
    "print()\n",
    "print(\"Loading Switzerland data.\")\n",
    "sdata = load_data('data5/processed.switzerland.data')\n",
    "print(\"Size:\", len(sdata), \"x\", len(sdata[0]))\n",
    "print(\"First row:\", sdata[0, :])\n",
    "smc = count_missing(sdata)\n",
    "print(smc, \"missing values.\")\n",
    "\n",
    "print()\n",
    "print(\"Loading VA data.\")\n",
    "vdata = load_data('data5/processed.va.data')\n",
    "print(\"Size:\", len(vdata), \"x\", len(vdata[0]))\n",
    "print(\"First row:\", vdata[0, :])\n",
    "vmc = count_missing(vdata)\n",
    "print(vmc, \"missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing(data):\n",
    "    clean = []\n",
    "    for row in data:\n",
    "        if '?' not in row:\n",
    "            clean.append(row)\n",
    "    return np.array(clean)\n",
    "\n",
    "\n",
    "def unique_vals(data):\n",
    "    natts = len(data[0])\n",
    "    ua = [[] for _ in range(natts)]\n",
    "    for row in data:\n",
    "        for i in range(natts):\n",
    "            v = row[i]\n",
    "            a = ua[i]\n",
    "            if v not in a:\n",
    "                a.append(v)\n",
    "    return [len(ua[i]) for i in range(natts)]\n",
    "\n",
    "\n",
    "def count_occurence(datacol):\n",
    "    cnt = Counter()\n",
    "    for value in datacol:\n",
    "        cnt[value] += 1\n",
    "    return dict(cnt)\n",
    "\n",
    "\n",
    "ccombo = remove_missing(np.concatenate((cdata, hdata, sdata, vdata)))\n",
    "print(\"Number of complete rows:\", len(ccombo))\n",
    "nua = unique_vals(ccombo)\n",
    "print(\"Number of unique values per column of those rows:\")\n",
    "print(nua)\n",
    "occsp = count_occurence(ccombo[:, -1])\n",
    "print(\"Occurences last/prediction column:\")\n",
    "print(occsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_split(data, ratio=0.7):\n",
    "    rand = np.copy(data)\n",
    "    np.random.shuffle(rand)\n",
    "    splidx = int(len(data) * ratio)\n",
    "    return (rand[:splidx, :], rand[splidx:, :])\n",
    "\n",
    "\n",
    "def x_y_split(data):\n",
    "    n = len(data)\n",
    "    nc = len(data[0])\n",
    "    yidx = nc - 1\n",
    "    yrow = np.array([data[i, yidx] != '0' for i in range(n)])\n",
    "    return data[:, : yidx], yrow[:]\n",
    "\n",
    "\n",
    "def disc_num_split(data, thres=20):\n",
    "    nua = unique_vals(data)\n",
    "    ncols = len(nua)\n",
    "    da = []\n",
    "    na = []\n",
    "    for i in range(ncols):\n",
    "        if nua[i] < thres:\n",
    "            da.append(i)\n",
    "        else:\n",
    "            na.append(i)\n",
    "    xn = data[:, da].astype(np.float).astype(int)\n",
    "    xf = data[:, na].astype(np.float)\n",
    "    return xn, xf\n",
    "\n",
    "\n",
    "(tset, vset) = validation_split(ccombo)\n",
    "(tx, ty) = x_y_split(tset)\n",
    "(vx, vy) = x_y_split(vset)\n",
    "(txd, txn) = disc_num_split(tx)\n",
    "print(len(tset))\n",
    "print(\"Discrete variables:\")\n",
    "print(txd)\n",
    "print()\n",
    "print(\"Continuous variables\")\n",
    "print(txn)\n",
    "print()\n",
    "print(\"Outcome vector:\")\n",
    "print(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(labels):\n",
    "    t = 0\n",
    "    n = len(labels)\n",
    "    for l in labels:\n",
    "        t += l == True\n",
    "    if t == 0:\n",
    "        return 0\n",
    "    if t == n:\n",
    "        return 1\n",
    "    return t / n\n",
    "\n",
    "\n",
    "def entropy_sub(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    return - p * math.log(p, 2) - (1 - p) * math.log((1 - p), 2)\n",
    "\n",
    "\n",
    "def entropy(labels):\n",
    "    return entropy_sub(ratio(labels))\n",
    "\n",
    "\n",
    "def split_entropy(labelsl, N):\n",
    "    s = 0\n",
    "    for labels in labelsl:\n",
    "        s += len(labels) / N * entropy(labels)\n",
    "    return s\n",
    "\n",
    "\n",
    "def information_gain(labels, indices):\n",
    "    seplabs = labels[indices], list(set(labels) - set(labels[indices]))\n",
    "    return entropy(labels) - split_entropy(seplabs, len(labels))\n",
    "\n",
    "\n",
    "def plot_entropy(N):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        l = [i >= j for j in range(N + 1)]\n",
    "        x.append(ratio(l))\n",
    "        y.append(entropy(l))\n",
    "    plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel(\"Entropy\")\n",
    "    plt.xlabel(\"Label ratio (or, probability of a true)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ey = entropy(ty)\n",
    "print(\"Entropy of training y:\", ey)\n",
    "plot_entropy(int(420 / 4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRow(object):\n",
    "    def __init__(self, discrete, numeric):\n",
    "        self.da = discrete\n",
    "        self.na = numeric\n",
    "\n",
    "    def get_discrete(self, index):\n",
    "        return self.da[index]\n",
    "\n",
    "    def get_numeric(self, index):\n",
    "        return self.na[index]\n",
    "\n",
    "    def size_discrete(self):\n",
    "        return len(self.da)\n",
    "\n",
    "    def size_numeric(self):\n",
    "        return len(self.na)\n",
    "\n",
    "\n",
    "def create_rows(discrete, numeric):\n",
    "    n = len(discrete)  # number of rows\n",
    "    ra = []\n",
    "    for i in range(n):\n",
    "        ra.append(DataRow(discrete[i], numeric[i]))\n",
    "    return ra\n",
    "\n",
    "\n",
    "tdra = create_rows(txd, txn)\n",
    "print(txd[4, 2], tdra[4].get_discrete(2))\n",
    "print(txn[4, 2], tdra[4].get_numeric(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, data, y, tree_type=0, thres=0.1):\n",
    "        \"\"\" Decision Tree creation, according to the follows the steps: \n",
    "            1. Stores the attributes\n",
    "                data - A vector of DataRow objects, each instance containing\n",
    "                       the discrete and numeric data for one patient\n",
    "                y - A vector of boolean class labels, each corresponding to a\n",
    "                    DataRow instance of a patient at the same index. \n",
    "                tree_type - 0: create the Tree with the highest IG every node \n",
    "                            1: create DiscreteTrees only\n",
    "                            2: create NumericTrees only\n",
    "                thres - The cutoff value for IG, to stop splitting the tree.\n",
    "                        Below this value the node becomes terminal and no further\n",
    "                        splits are made.\n",
    "            2. Computes the optimal discrete and numeric splits and converts itself\n",
    "                to correct type based on the tree_type parameter.\n",
    "            3. The Information Gain of the split is compared to the thres parameter,\n",
    "                to determine if the node is terminal. If the node is not terminal,\n",
    "                new subtrees are created for each distribution resulting from the\n",
    "                split. \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.y = y\n",
    "        self.tree_type = tree_type\n",
    "        self.thres = thres\n",
    "        # TODO: that continuous stuff\n",
    "        if tryingNumeric is True:\n",
    "            self.convert_tree(NumericTree(self))\n",
    "        else:\n",
    "            self.convert_tree(DiscreteTree(self))\n",
    "        self.terminal = self.ig < thres\n",
    "        if not self.terminal:\n",
    "            self.create_subtrees()\n",
    "\n",
    "    def store_split_values(self, split_variable, indices, ig, vals):\n",
    "        \"\"\" Sets the values of the passed parameters as object attributes.\n",
    "            Also sets the result label for this node, based on the set of y-values.\n",
    "                split_variable - The index of variable on which the split was based \n",
    "                indices - The list of index lists, each corresponding to the\n",
    "                            indices for a subset resulting from the split\n",
    "                ig - Information Gain computed from the split\"\"\"\n",
    "        self.split_variable = split_variable\n",
    "        self.indices = indices\n",
    "        self.ig = ig\n",
    "        self.vals = vals\n",
    "        self.ratli = []\n",
    "        self.outli = []\n",
    "        maxlen = 0\n",
    "        for valind in indices:\n",
    "            r = ratio(self.y[valind])\n",
    "            self.ratli.append(r)\n",
    "            self.outli.append(r > 0.5)\n",
    "\n",
    "    def convert_tree(self, new_tree):\n",
    "        \"\"\" Converts this object to the tree passed as the new_tree parameter.\n",
    "            All attributes from the new_tree are transfered.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\n",
    "            new_tree - Either a DiscreteTree or a NumericTree instance, to which\n",
    "                        this object is converted\"\"\"\n",
    "        self.__class__ = new_tree.__class__\n",
    "        self.__dict__ = new_tree.__dict__\n",
    "\n",
    "    def create_subtrees(self):\n",
    "        \"\"\" Based on the indices parameter stored after the split, the different\n",
    "            subsets of the data are created and for each a new DecisionTree made.\n",
    "            The DecisionTrees are stored in a dict mapping the variable value from the\n",
    "            split to the DecisionTree created by selecting that value for the split\"\"\"\n",
    "        self.branches = {}\n",
    "        nindices = len(self.indices)\n",
    "        for i in range(nindices):\n",
    "            thesedice = self.indices[i]\n",
    "            newx = self.data[thesedice]\n",
    "            newy = self.y[thesedice]\n",
    "            nt = DecisionTree(newx, newy)\n",
    "            self.branches[self.vals[i]] = nt\n",
    "\n",
    "    def classify(self, row):\n",
    "        \"\"\" Returns the most common label for the values stored in the row, based on\n",
    "            the splits in the DecisionTree.\n",
    "            row - The DataRow object containing the values that are being\n",
    "                    classified\"\"\"\n",
    "        i = self.indices.index(max(self.indices, key=len))\n",
    "        mcommon = self.outli[i]\n",
    "\n",
    "        if self.terminal:\n",
    "            v = row.get_discrete(self.split_variable)  # TODO: hard-coded\n",
    "            if v in self.vals:\n",
    "                i = self.vals.index(v)\n",
    "                return self.outli[i]\n",
    "            else:\n",
    "                return mcommon\n",
    "        else:\n",
    "            subtree = self.get_subtree(row)\n",
    "            if subtree == None:\n",
    "                return mcommon\n",
    "            else:\n",
    "                return subtree.classify(row)\n",
    "\n",
    "    def validate(self, data, y):\n",
    "        \"\"\" Classifies all the DataRow instances in data and compares the outcome to \n",
    "            the labels specified in y. Returns the percentage of elements that was\n",
    "            classified correctly.\n",
    "            data - List of DataRow instances from the validation set.\n",
    "            y - List of boolean labels each belonging to a DataRow instances at the\n",
    "                same index\"\"\"\n",
    "        disc, num = disc_num_split(data)\n",
    "        superdata = create_rows(disc, num)\n",
    "        results = []\n",
    "        for row in superdata:\n",
    "            results.append(self.classify(row))\n",
    "        results = np.array(results)\n",
    "        yippie = 0\n",
    "        for x in range(len(results)):\n",
    "            yippie += results[x] == y[x]\n",
    "        return yippie / len(y) * 100\n",
    "\n",
    "    def split(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_subtree(self, instance):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class DiscreteTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            discrete variable to split this subset of the data on.\n",
    "            dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                    DiscreteTree instance.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best discrete variable to split the current dataset on,\n",
    "            based on the resulting IG. For this best split variable, it stores\n",
    "            the following values as object attributes:\n",
    "                1. The index of the variable on which the split is based\n",
    "                2. List of lists containing indices for the split\n",
    "                3. IG of the split\n",
    "                4. List of unique values for the split variable\"\"\"\n",
    "        nrows = len(self.data)\n",
    "        natts = self.data[0].size_discrete()\n",
    "        maxgain = bestatt = float('-inf')\n",
    "        bestindices = []\n",
    "        for att in range(natts):\n",
    "            attvals = {}\n",
    "            for i in range(nrows):\n",
    "                v = self.data[i].get_discrete(att)\n",
    "                if v not in attvals:\n",
    "                    attvals[v] = [i]\n",
    "                else:\n",
    "                    attvals[v].append(i)\n",
    "            gain = 0\n",
    "            values = attvals.keys()\n",
    "            for val in values:\n",
    "                indices = attvals[val]\n",
    "                gain += information_gain(self.y, indices)\n",
    "            gain /= len(values)\n",
    "            if gain > maxgain:\n",
    "                maxgain = gain\n",
    "                bestatt = att\n",
    "                bestindices = attvals\n",
    "        self.store_split_values(bestatt, list(bestindices.values()),\n",
    "                                maxgain, list(bestindices.keys()))\n",
    "\n",
    "    def get_subtree(self, row):\n",
    "        \"\"\" Returns the subtree one branch down corresponding the to value of\n",
    "            variable on which the split at this node was performed.\n",
    "            Returns None if the value was not present at the split.\n",
    "            row - The DataRow object containing the values that are being\n",
    "                    classified\"\"\"\n",
    "        #v = row[self.split_variable]\n",
    "        v = row.get_discrete(self.split_variable)\n",
    "        if v in self.vals:\n",
    "            i = self.vals.index(v)\n",
    "            if i in self.branches.keys():\n",
    "                return self.branches[i]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "tryingNumeric = False\n",
    "ditr = DecisionTree(tdra, ty)\n",
    "print(\"Accuracy (%) using discrete only:\")\n",
    "print(ditr.validate(vx, vy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "class NumericTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            discrete variable to split this subset of the data on.\n",
    "            dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                    NumericTree instance.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best boundary for any numeric variable to split the\n",
    "            current dataset on, based on the resulting IG. For this best split\n",
    "            boundary, it stores the following values as object attributes:\n",
    "                1. The index of the variable on which the split is based\n",
    "                2. List of lists containing indices for the split\n",
    "                3. IG of the split\n",
    "                4. The value of the boundary for the split\n",
    "                5. A value list containing [0, 1]\"\"\"\n",
    "        # TODO\n",
    "        nrows = len(self.data)\n",
    "        natts = self.data[0].size_numeric()\n",
    "        maxgain = bestatt = float('-inf')\n",
    "        bestindices = []\n",
    "        for att in range(natts):\n",
    "            attvals = []\n",
    "            for i in range(nrows):\n",
    "                v = self.data[i].get_numeric(att)\n",
    "                attvals.append(v)\n",
    "            minv = min(attvals)\n",
    "            maxv = max(attvals)\n",
    "            rng = np.linspace(minv, maxv, nrows)\n",
    "\n",
    "            for thr in rng:\n",
    "                gain = 0\n",
    "                indices = []\n",
    "                for i in range(nrows):\n",
    "                    thingy = attvals[i]\n",
    "                    if thingy < thr:\n",
    "                        indices.append(i)\n",
    "                gain = information_gain(self.y, indices)\n",
    "                if gain > maxgain:\n",
    "                    maxgain = gain\n",
    "                    bestatt = att\n",
    "                    bestindices = indices\n",
    "        self.store_split_values(bestatt, attvals,\n",
    "                                maxgain, indices)\n",
    "\n",
    "    def get_subtree(self, row):\n",
    "        \"\"\" Returns the subtree one branch down based on the value of\n",
    "            variable on which the split at this node was performed and the\n",
    "            boundary used for this split.\n",
    "            row - The DataRow object containing the values that are being\n",
    "                    classified\"\"\"\n",
    "        # TODO: check\n",
    "        v = row.get_numeric(self.split_variable)\n",
    "        if v > self.vals:  # == threshold\n",
    "            i = self.vals.index(v)\n",
    "            if i in self.branches.keys():\n",
    "                return self.branches[i]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "tryingNumeric = True\n",
    "nitr = DecisionTree(tdra, ty) # I kinda gave up, this HW is so long!\n",
    "tryingNumeric = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    (tset, vset) = validation_split(data)\n",
    "    (tx, ty) = x_y_split(tset)\n",
    "    (txd, txn) = disc_num_split(tx)\n",
    "    tdra = create_rows(txd, txn)\n",
    "    (tx, ty) = x_y_split(tset)\n",
    "    ttree = DecisionTree(tdra, ty)\n",
    "    (vx, vy) = x_y_split(vset)\n",
    "    (vxd, vxn) = disc_num_split(vx)\n",
    "    # The validate handles the disc/num and DataRow stuff\n",
    "    return ttree.validate(vx, vy)\n",
    "\n",
    "\n",
    "tryingNumeric = False\n",
    "split_data(ccombo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis questions [2 pts]\n",
    "\n",
    "If your algorithm is correct and you averaged over enough different validation splits, you might see some strange results in the comparison you just produced. For the last part of the assignment, answer these questions about the results. Write you answers below each question inside this cell.\n",
    "\n",
    "#### Why does the validation score for the training for numeric and combined splits eventually reach 100% correct, but never for just discrete splits?\n",
    "\n",
    "Could be there's the discrete data's not complete and/or consistent enough to make 100% accuracy possible. The tree will always be impure.\n",
    "\n",
    "#### Can you explain how it is possible that the value of just the discrete variables is higher than the value of the discrete and numeric variables combined? What property of the algorithm makes this outcome possible?\n",
    "\n",
    "The validation split splits the data randomly. Therefore the outcome is somewhat random as well.\n",
    "\n",
    "#### What is your hypothesis for why this happens for this particular data? What could you do to improve the validation results?\n",
    "\n",
    "People are more complicated than 13 variables, etc. There are exceptions to most rules when dealing with something so complex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
