cat("Train MSE:", mean(lsq$residuals ^ 2),
"\nTest MSE:", lsqmse, "\n")
total_ss <- sum((test[, 14] - mean(test[, 14])) ^ 2)
lsq_reg_ss <- lsqmse * length(test[, 14])
(R2 <- 1 - lsq_reg_ss / total_ss)
# Chunk 8
set.seed(1)
s_train <- train[sample(nrow(train)),]
medv <- which(colnames(s_train) == "MEDV")
medv <- 14 # now no longer shuffling columns
K <- 5
folds <- cut(seq(1, nrow(s_train)), breaks = K, labels = FALSE)
cv_mse <- numeric(K)
for (i in 1:K) {
test_indices <- which(folds == i, arr.ind = TRUE)
test_fold <- s_train[test_indices,]
train_folds <- s_train[-test_indices,]
lsq_i <- lm(MEDV ~ ., train_folds)
cv_mse[i] <- mean((test_fold[, 14] - predict(lsq_i, test_fold)) ^ 2)
}
lsq_5cv_mse <- mean(cv_mse)
cat("5-fold cross-validation MSE:", lsq_5cv_mse, "\n")
# Chunk 9
# install.packages("leaps")
library(leaps)
to_lm_input <- function(regressors) {
# Like everything else, this is not meant to be especially robust.
if ("(Intercept)" %in% regressors) {
regressors <- regressors[-which(regressors == "(Intercept)")]
} else {
regressors <- c(regressors, "-1")
}
regressorsp <- paste(regressors, collapse = " + ", sep = "")
return(paste("MEDV ~ ", regressorsp, sep = " "))
}
subset_5cv_mse <- matrix(nrow = K, ncol = 13)
for (k in 1:K) {
test_indices <- which(folds == k, arr.ind = TRUE)
test_fold <- s_train[test_indices,]
train_folds <- s_train[-test_indices, ]
rsti <- regsubsets(x = train_folds[, -14], y = train_folds[, 14],
nvmax = 13, method = "exhaustive")
for (n in 1:13) {
regressors <- names(which(summary(rsti)$which[n,]))
lm_kn <- lm(to_lm_input(regressors), train_folds)
subset_5cv_mse[k, n] <-
mean((test_fold[, 14] - predict(lm_kn, test_fold)) ^ 2)
}
}
(n_5cv_mse <- apply(subset_5cv_mse, 2, mean))
(nvars <- which.min(n_5cv_mse))
subset_5cv_mse <- matrix(nrow = K, ncol = 13)
for (k in 1:K) {
test_indices <- which(folds == k, arr.ind = TRUE)
test_fold <- s_train[test_indices,]
train_folds <- s_train[-test_indices, ]
rsti <- regsubsets(x = train_folds[, -14], y = train_folds[, 14],
nvmax = 13, method = "exhaustive")
for (n in 1:13) {
regressors <- names(which(summary(rsti)$which[n,]))
lm_kn <- lm(to_lm_input(regressors), train_folds)
subset_5cv_mse[k, n] <-
mean((test_fold[, 14] - predict(lm_kn, test_fold)) ^ 2)
}
}
(subset_5cv_mse_per_n <- apply(subset_5cv_mse, 2, mean))
(best_5cv_n <- which.min(subset_5cv_mse_per_n))
library(MASS)
?lm.ridge
coef(ridge)
select(lm.ridge(MEDV ~ ., train, lambda = seq(0, 10, .001)))
select(lm.ridge(MEDV ~ ., train, lambda = seq(3.5, 4.5, 0.0001)))
ridge_gcv_lambda <- 3.96
ridge <- lm.ridge(MEDV ~ ., train, lambda = ridge_gcv_lambda)
# install.packages("lars")
library(lars)
set.seed(1)
X = as.matrix(train[,1:13])
y = as.matrix(train[,14])
lambda_range <- seq(0, 10, .2)
cvs1 <- numeric(length(lambda_range))
for (i in 1:length(lambda_range)) {
cvs1[i] <- min(cv.lars(X, y, K = 5, type = "lasso",
s = lambda_range[i], plot.it = FALSE)$cv)
}
plot(cvs1)
# Try good region again:
lambda_range <- seq(20 * .2, 40 * .2, .01); cvs2 <- numeric(length(lambda_range))
for (i in 1:length(lambda_range)) {
cvs2[i] <- min(cv.lars(lambda_range, y, K = 5, type = "lasso",
s = lambda_range[i], plot.it = FALSE)$cv)
}
plot(cvs2)
cbind(min(cvs1, min(cvs2)))
cbind(min(cvs1), min(cvs2))
set.seed(1)
X = as.matrix(train[,1:13])
y = as.matrix(train[,14])
lambda_range1 <- seq(0, 10, .2)
cvs1 <- numeric(length(lambda_range1))
for (i in 1:length(lambda_range1)) {
cvs1[i] <- min(cv.lars(X, y, K = 5, type = "lasso",
s = lambda_range1[i], plot.it = FALSE)$cv)
}
plot(cvs1)
# Try good region again:
lambda_range2 <- seq(20 * .2, 40 * .2, .01)
cvs2 <- numeric(length(lambda_range2))
for (i in 1:length(lambda_range2)) {
cvs2[i] <- min(cv.lars(X, y, K = 5, type = "lasso",
s = lambda_range2[i], plot.it = FALSE)$cv)
}
plot(cvs2)
cbind(min(cvs1), min(cvs2))
(best_lambda <- lambda_range2[which.min(cvs2)])
lasso <- lars(X, y, type = "lasso")
predobj <- predict.lars(lasso, as.matrix(test[,1:13]),
mode = "lambda", s = best_lambda)
lassopreds <- predobj$fit
lassomse <- mean((test[, 14] - lassopreds) ^ 2)
cat("Test MSE:", lassomse, "\n")
(lasso_5cv_lambda <- lambda_range2[which.min(cvs2)])
set.seed(1)
library(glmnet)
lasso2 <- cv.glmnet(lambda_range, y,
alpha = 1, nfolds = 5, lambda = seq(0, 10, .001))
lasso2_5cv_lambda <- lasso2$lambda.min
(cv_lambdas <- c(lasso_5cv_lambda, lasso2_5cv_lambda)) # hmm...
lasso2 <- cv.glmnet(X, y,
alpha = 1, nfolds = 5, lambda = seq(0, 10, .001))
lasso2_5cv_lambda <- lasso2$lambda.min
(cv_lambdas <- c(lasso_5cv_lambda, lasso2_5cv_lambda)) # hmm...
predlasso2 <- predict(lasso2, s = lasso2_5cv_lambda,
newx = as.matrix(test[,1:13]))
cat("Test MSE:", mean((test[, 14] - predlasso2) ^ 2), "\n")
cat("Test MSE:", mean((test[, 14] - predlasso2) ^ 2),
"(compare to ", lasso_mse, "above)\n")
lasso_mse <- mean((test[, 14] - lassopreds) ^ 2)
cat("Test MSE:", lasso_mse, "\n")
cat("Test MSE:", mean((test[, 14] - predlasso2) ^ 2),
"(compare to ", lasso_mse, "above)\n")
(cv_lambdas <- c(las.so_5cv_lambda, lasso2_5cv_lambda)) # hmm...
(cv_lambdas <- c(lasso_5cv_lambda, lasso2_5cv_lambda)) # hmm...
cat("Test MSE:", mean((test[, 14] - predlasso2) ^ 2),
"(compare to", lasso_mse, "above)\n")
# Mixing lambda's:
predobj2 <- predict.lars(lasso, as.matrix(test[,1:13]),
mode = "lambda", s = lasso2_5cv_lambda)
lasso_mse2 <- mean((test[, 14] - predobj2$fit) ^ 2)
cat("Test MSE:", lasso_mse2, "\n")
# Then let's try this:
lambda_range <- c(lasso_5cv_lambda, lasso2_5cv_lambda)
min(cv.lars(lambda_range, y, K = 5, type = "lasso",
s = lasso_5cv_lambda, plot.it = FALSE)$cv)
# Then let's try this:
min(cv.lars(lambda_range, y, K = 5, type = "lasso",
s = lasso_5cv_lambda, plot.it = FALSE)$cv)
# Then let's try this:
min(cv.lars(X, y, K = 5, type = "lasso",
s = lasso_5cv_lambda, plot.it = FALSE)$cv)
min(cv.lars(X, y, K = 5, type = "lasso",
s = lasso2_5cv_lambda, plot.it = FALSE)$cv)
set.seed(1)
library(glmnet)
lasso2 <- cv.glmnet(X, y,
alpha = 1, nfolds = 5, lambda = seq(0, 10, .001))
lasso2_5cv_lambda <- lasso2$lambda.min
(cv_lambdas <- c(lasso_5cv_lambda, lasso2_5cv_lambda)) # hmm...
predlasso2 <- predict(lasso2, s = lasso2_5cv_lambda,
newx = as.matrix(test[,1:13]))
cat("Test MSE:", mean((test[, 14] - predlasso2) ^ 2),
"(compare to", lasso_mse, "above)\n")
# Mixing lambda's:
predobj2 <- predict.lars(lasso, as.matrix(test[,1:13]),
mode = "lambda", s = lasso2_5cv_lambda)
lasso_mse2 <- mean((test[, 14] - predobj2$fit) ^ 2)
cat("Test MSE:", lasso_mse2, "\n")
# That's lower, ofc possible since my earlier lambda search isn't perfect.
# Then let's try this:
min(cv.lars(X, y, K = 5, type = "lasso",
s = lasso_5cv_lambda, plot.it = FALSE)$cv)
min(cv.lars(X, y, K = 5, type = "lasso",
s = lasso2_5cv_lambda, plot.it = FALSE)$cv)
# CV error using the provided function _is_ lower for the s we found earlier --
# we'll keep using it (so as not to use our test set in training).
lasso2_5cv_lambda <- s # not used again, but left more confusing otherwise.
cat(
" Least squares on full train set: R2 =",
1 - lsq_mse * nrow(test) / total_ss,
"\n",
"Best subset lsq (subset found with 5-fold cv): R2 =",
1 - subset_mse * nrow(test) / total_ss,
"\n",
"Ridge (shrinkage found with 5-fold cv): R2 =",
1 - ridge_mse * nrow(test) / total_ss,
"\n",
"Lasso (shrinkage found with 5-fold cv): R2 =",
1 - lasso_mse * nrow(test) / total_ss,
"\n",
"Mean of MEDV: ",
mean(data[, 14]),
"\n",
"Std dev of MEDV: ",
sd(data[, 14])
)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/Bram/Google Drive/MSc/SL/a1")
setwd("C:/Users/Bram/Google Drive/MSc/SL/a1")
rm(list = ls())
# Chunk 2
data <- read.table("housing-p.data", header = TRUE)
dim(data)
head(data)
str(data)
# data$CHAS <- as.factor(data$CHAS) # the models should already do this
train <- data[1:350,]
test <- data[351:nrow(data),]
all.equal(data, rbind(train, test))
# Chunk 3
lsq <- lm(MEDV ~ ., train)
# Chunk 4
summary(lsq)
# Chunk 5
par(mfrow = c(1,2))
for (i in 1:2) { # change end to 13 for all (i.e. too many for a pdf) plots
lsq_i <- lm(paste("MEDV ~ ", colnames(data)[i]), data)
plot(data[, i], data$MEDV, xlab = i,
main = paste("MSE: ", round(mean(lsq_i$residuals ^ 2), 2),
"\nME: ", round(mean(lsq_i$residuals), 2)))
abline(lsq_i)
}
par(mfrow = c(1,1))
# Chunk 6
# install.packages('usdm')
library(usdm)
vif(data[, 1:13])
# Chunk 7
lsq_predictions <- predict(lsq, test)
lsq_mse <- mean((test[, 14] - lsq_predictions) ^ 2)
cat("Train MSE:", mean(lsq$residuals ^ 2),
"\nTest MSE:", lsq_mse, "\n")
total_ss <- sum((test[, 14] - mean(test[, 14])) ^ 2)
lsq_reg_ss <- lsq_mse * length(test[, 14])
(lsq_R2 <- 1 - lsq_reg_ss / total_ss)
# Chunk 8
set.seed(1)
s_train <- train[sample(nrow(train)),]
medv <- which(colnames(s_train) == "MEDV")
medv <- 14 # now no longer shuffling columns
K <- 5
folds <- cut(seq(1, nrow(s_train)), breaks = K, labels = FALSE)
cv_mse <- numeric(K)
for (i in 1:K) {
test_indices <- which(folds == i, arr.ind = TRUE)
test_fold <- s_train[test_indices,]
train_folds <- s_train[-test_indices,]
lsq_i <- lm(MEDV ~ ., train_folds)
cv_mse[i] <- mean((test_fold[, 14] - predict(lsq_i, test_fold)) ^ 2)
}
lsq_5cv_mse <- mean(cv_mse)
cat("5-fold cross-validation MSE:", lsq_5cv_mse, "\n")
# Chunk 9
# install.packages("leaps")
library(leaps)
to_lm_input <- function(regressors) {
# Like everything else, this is not meant to be especially robust.
if ("(Intercept)" %in% regressors) {
regressors <- regressors[-which(regressors == "(Intercept)")]
} else {
regressors <- c(regressors, "-1")
}
regressorsp <- paste(regressors, collapse = " + ", sep = "")
return(paste("MEDV ~ ", regressorsp, sep = " "))
}
# Chunk 10
subset_5cv_mse <- matrix(nrow = K, ncol = 13)
for (k in 1:K) {
test_indices <- which(folds == k, arr.ind = TRUE)
test_fold <- s_train[test_indices,]
train_folds <- s_train[-test_indices, ]
rsti <- regsubsets(x = train_folds[, -14], y = train_folds[, 14],
nvmax = 13, method = "exhaustive")
for (n in 1:13) {
regressors <- names(which(summary(rsti)$which[n,]))
lm_kn <- lm(to_lm_input(regressors), train_folds)
subset_5cv_mse[k, n] <-
mean((test_fold[, 14] - predict(lm_kn, test_fold)) ^ 2)
}
}
(subset_5cv_mse_per_n <- apply(subset_5cv_mse, 2, mean))
(best_5cv_n <- which.min(subset_5cv_mse_per_n))
# Chunk 11
train_subset <- regsubsets(x = train[, -14], y = train[, 14],
nvmax = best_5cv_n, method = "exhaustive")
best_5cv_n_regressors <- names(which(summary(train_subset)$which[best_5cv_n,]))
lm_subset <- lm(to_lm_input(best_5cv_n_regressors), train)
subset_predictions <- predict(lm_subset, test)
subset_mse <- mean((test[, 14] - subset_predictions) ^ 2)
cat("Test MSE:", subset_mse, "\n")
# Chunk 12
library(MASS)
# Chunk 13
select(lm.ridge(MEDV ~ ., train, lambda = seq(0, 10, .001)))
select(lm.ridge(MEDV ~ ., train, lambda = seq(3.5, 4.5, 0.0001)))
ridge_gcv_lambda <- 3.96
ridge <- lm.ridge(MEDV ~ ., train, lambda = ridge_gcv_lambda)
ridge_predictions <- as.matrix(cbind(const = 1, test[-14])) %*% coef(ridge)
ridge_mse <- mean((test[, 14] - ridge_predictions) ^ 2)
cat("Test MSE:", ridge_mse, "\n")
# Chunk 14
# install.packages("lars")
library(lars)
# Chunk 15
set.seed(1)
X = as.matrix(train[,1:13])
y = as.matrix(train[,14])
lambda_range1 <- seq(0, 10, .2)
cvs1 <- numeric(length(lambda_range1))
for (i in 1:length(lambda_range1)) {
cvs1[i] <- min(cv.lars(X, y, K = 5, type = "lasso",
s = lambda_range1[i], plot.it = FALSE)$cv)
}
plot(cvs1)
# Try good region again:
lambda_range2 <- seq(20 * .2, 40 * .2, .01)
cvs2 <- numeric(length(lambda_range2))
for (i in 1:length(lambda_range2)) {
cvs2[i] <- min(cv.lars(X, y, K = 5, type = "lasso",
s = lambda_range2[i], plot.it = FALSE)$cv)
}
plot(cvs2)
cbind(min(cvs1), min(cvs2))
(lasso_5cv_lambda <- lambda_range2[which.min(cvs2)])
lasso <- lars(X, y, type = "lasso")
predobj <- predict.lars(lasso, as.matrix(test[,1:13]),
mode = "lambda", s = lasso_5cv_lambda)
lassopreds <- predobj$fit
lasso_mse <- mean((test[, 14] - lassopreds) ^ 2)
cat("Test MSE:", lasso_mse, "\n")
# Chunk 16
set.seed(1)
library(glmnet)
lasso2 <- cv.glmnet(X, y,
alpha = 1, nfolds = 5, lambda = seq(0, 10, .001))
lasso2_5cv_lambda <- lasso2$lambda.min
(cv_lambdas <- c(lasso_5cv_lambda, lasso2_5cv_lambda)) # hmm...
predlasso2 <- predict(lasso2, s = lasso2_5cv_lambda,
newx = as.matrix(test[,1:13]))
cat("Test MSE:", mean((test[, 14] - predlasso2) ^ 2),
"(compare to", lasso_mse, "above)\n")
# Mixing lambda's:
predobj2 <- predict.lars(lasso, as.matrix(test[,1:13]),
mode = "lambda", s = lasso2_5cv_lambda)
lasso_mse2 <- mean((test[, 14] - predobj2$fit) ^ 2)
cat("Test MSE:", lasso_mse2, "\n")
# That's lower, ofc possible since my earlier lambda search isn't perfect.
# Then let's try this:
min(cv.lars(X, y, K = 5, type = "lasso",
s = lasso_5cv_lambda, plot.it = FALSE)$cv)
min(cv.lars(X, y, K = 5, type = "lasso",
s = lasso2_5cv_lambda, plot.it = FALSE)$cv)
# CV error using the provided function _is_ lower for the s we found earlier --
# we'll keep using it (so as not to use our test set in training).
cat(
" Least squares on full train set: R2 =",
1 - lsq_mse * nrow(test) / total_ss,
"\n",
"Best subset lsq (subset found with 5-fold cv): R2 =",
1 - subset_mse * nrow(test) / total_ss,
"\n",
"Ridge (shrinkage found with 5-fold cv): R2 =",
1 - ridge_mse * nrow(test) / total_ss,
"\n",
"Lasso (shrinkage found with 5-fold cv): R2 =",
1 - lasso_mse * nrow(test) / total_ss,
"\n",
"Mean of MEDV: ",
mean(data[, 14]),
"\n",
"Std dev of MEDV: ",
sd(data[, 14])
)
best_5cv_n
ridge_gcv_lambda
cbind(coef(ridge), coef(lsq))
cbind(coef(lsq), coef(ridge), coef(lasso))
cbind(coef(lsq), coef(ridge))
coef(lasso)
best_5cv_n
ridge_gcv_lambda
cbind(coef(lsq), coef(ridge))
?scale
y
scale(y)
scale(X)
X <- scale(X)
X
str(X)
X[1,1]
scale(train)
ridge <- lm.ridge(MEDV ~ ., scale(train), lambda = ridge_gcv_lambda)
as.data.frame(scale(train))
ridge <- lm.ridge(MEDV ~ ., as.data.frame(scale(train)), lambda = ridge_gcv_lambda)
ridge_predictions <- as.matrix(cbind(const = 1, test[-14])) %*% coef(ridge)
ridge_mse <- mean((test[, 14] - ridge_predictions) ^ 2)
cat("Test MSE:", ridge_mse, "\n")
predict(ridge, test[, 14])
predict.lmridge(ridge, test[, 14])
?lmridge
best_5cv_n
ridge_gcv_lambda
cbind(coef(lsq), coef(ridge))
select(lm.ridge(MEDV ~ ., train, lambda = seq(0, 10, .001)))
select(lm.ridge(MEDV ~ ., train, lambda = seq(3.5, 4.5, 0.0001)))
ridge_gcv_lambda <- 3.96
ridge <- lm.ridge(MEDV ~ ., train, lambda = ridge_gcv_lambda)
ridge_predictions <- as.matrix(cbind(const = 1, test[-14])) %*% coef(ridge)
ridge_mse <- mean((test[, 14] - ridge_predictions) ^ 2)
cat("Test MSE:", ridge_mse, "\n")
best_5cv_n
ridge_gcv_lambda
cbind(coef(lsq), coef(ridge))
summary(test$NOX)
sd(test$NOX)
sd(data$NOX)
str(data$NOX)
summary(data$NOX)
mean(data$MEDV)
cbind(coef(lsq), coef(ridge), ridge$coef)
plot(data$AGE, data$MEDV)
boxplot(data$AGE, data$MEDV)
boxplot(data$B, data$MEDV)
plot(data$B, data$MEDV)
plot(data$B)
summary(data$B)
?IQR
ISQ(data$CRIM)
IQR(data$CRIM)
IQR(data$B)
IQR(data[,1:13])
apply(data[, 1:13], 2, IQR)
cbind(coef(lsq), coef(ridge), apply(data[, 1:13], 2, IQR))
cbind(coef(lsq), coef(ridge), c(0, apply(data[, 1:13], 2, IQR)))
cbind(coef(lsq), coef(ridge), c(NA, apply(data[, 1:13], 2, IQR)))
cbind(coef(lsq), coef(ridge), c(NA, apply(data[, 1:13], 2, median)))
?apply
?mean
cbind(coef(lsq), coef(ridge), c(NA, apply(data[, 1:13], 2, mean, trim = .2)))
cbind(coef(lsq), coef(ridge), c(mean(data[, 14], trim = .2), apply(data[, 1:13], 2, mean, trim = .2)))
cbind(coef(lsq), coef(ridge), c(mean(data[, 14], trim = .2),
apply(data[, 1:13], 2, mean, trim = .2)),
c(sd(data[, 14]), apply(data[, 1:13], 2, sd)))
cbind(coef(lsq), coef(ridge), c(NA,
apply(data[, 1:13], 2, mean, trim = .2)),
c(NA, apply(data[, 1:13], 2, sd)))
cbind("LSQ" = coef(lsq), coef(ridge), c(NA,
apply(data[, 1:13], 2, mean, trim = .2)),
c(NA, apply(data[, 1:13], 2, sd)))
cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge), c(NA,
".2 trimmed mean" = apply(data[, 1:13], 2, mean, trim = .2)),
"sd"= c(NA, apply(data[, 1:13], 2, sd)))
cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge), ".2 trimmed mean" = c(NA,
apply(data[, 1:13], 2, mean, trim = .2)),
"sd"= c(NA, apply(data[, 1:13], 2, sd)))
cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, 1:13], 2, mean, trim = .2)),
"sd" = c(NA, apply(data[, 1:13], 2, sd)))
cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, -14], 2, mean, trim = .2)),
"sd (both of data$column)" = c(NA, apply(data[, -14], 2, sd)),
"ridge coef * .2 tr mean" =  t(coef(ridge)) %*% c(NA, apply(data[, -14], 2, mean, trim = .2)))
cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, -14], 2, mean, trim = .2)),
"sd (both of data$column)" = c(NA, apply(data[, -14], 2, sd)),
"ridge coef * .2 tr mean" =  coef(ridge) * c(NA, apply(data[, -14], 2, mean, trim = .2)))
cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, -14], 2, mean, trim = .2)),
"sd" = c(NA, apply(data[, -14], 2, sd)),
"Ridge * .2 tr mean" =  coef(ridge) * c(NA, apply(data[, -14], 2, mean, trim = .2)))
cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, -14], 2, mean, trim = .2)),
"Std dev" = c(NA, apply(data[, -14], 2, sd)),
"Ridge * .2 tr mean" =  coef(ridge) * c(NA, apply(data[, -14], 2,
mean, trim = .2)))
round(cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, -14], 2, mean, trim = .2)),
"Std dev" = c(NA, apply(data[, -14], 2, sd)),
"Ridge * .2 tr mean" =  coef(ridge) * c(NA, apply(data[, -14], 2,
mean, trim = .2))), 2)
round(cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, -14], 2, mean, trim = .2)),
"Std dev" = c(NA, apply(data[, -14], 2, sd)),
"Ridge * .2m" =  coef(ridge) * c(NA, apply(data[, -14], 2,
mean, trim = .2))), 2)
best_5cv_n_regressors
best_5cv_n_regressors # missing: AGE, INDUS
best_5cv_n
ridge_gcv_lambda
round(cbind("LSQ" = coef(lsq), "Ridge" = coef(ridge),
".2 tr mean" = c(NA, apply(data[, -14], 2, mean, trim = .2)),
"Std dev" = c(NA, apply(data[, -14], 2, sd)),
"Ridge * .2m" =  coef(ridge) * c(NA, apply(data[, -14], 2,
mean, trim = .2))), 2)
train_subset <- regsubsets(x = train[, -14], y = train[, 14],
nvmax = best_5cv_n, method = "exhaustive")
best_5cv_n_regressors <- names(which(summary(train_subset)$which[best_5cv_n,]))
lm_subset <- lm(to_lm_input(best_5cv_n_regressors), train)
subset_predictions <- predict(lm_subset, test)
subset_mse <- mean((test[, 14] - subset_predictions) ^ 2)
cat("Test MSE:", subset_mse, "\n")
best_5cv_n_regressors
