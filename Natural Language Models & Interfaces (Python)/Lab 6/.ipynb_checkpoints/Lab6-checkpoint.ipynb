{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lennart Beekhuis\n",
    "# Bram Otten, 10992456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**\n",
    "\n",
    "* Context-Free Grammars\n",
    "    * [Symbols](#symbols)\n",
    "    * [Rules](#rules)\n",
    "    * [Grammar](#grammar)\n",
    "    * [Generation](#generation)\n",
    "* Recogniser\n",
    "    * [Shift-Reduce](#shiftreduce)\n",
    "    * [CKY](#cky)\n",
    "* PCFGs\n",
    "    * [Definition](#pcfg)\n",
    "    * [MLE](#mle)\n",
    "    \n",
    "    \n",
    "**Table of Exercises**\n",
    "\n",
    "* Theory (10 points)\n",
    "    * [Exercise 6-2](#ex6-2)\n",
    "    * [Exercise 6-4](#ex6-4)\n",
    "    * [Exercise 6-5](#ex6-5)\n",
    "    * [Exercise 6-7](#ex6-7)\n",
    "    * [Exercise 6-8](#ex6-8)\n",
    "* Practice (22 points)    \n",
    "    * [Exercise 6-1](#ex6-1)\n",
    "    * [Exercise 6-3](#ex6-3)\n",
    "    * [Exercise 6-6](#ex6-6)\n",
    "    * [Exercise 6-9](#ex6-9)\n",
    "    * [Exercise 6-10](#ex6-10)\n",
    "    * [Exercise 6-11](#ex6-11)\n",
    "    \n",
    "\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$\n",
    "* Use python3.\n",
    "* Use NLTK to read annotated data.\n",
    "* **Document your code**: TAs are more likely to understand the steps if you document them. If you don't, it's also difficult to give you partial points for exercises that are not completely correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"symbols\">  Symbols\n",
    "\n",
    "Context-free grammars manipulate **symbols**, namely, **terminals** (or constants) and **nonterminals** (or variables).\n",
    "\n",
    "For us, a **word** is an instance of a *terminal symbol* and **part-of-speech categories** as well as **phrasal categories** are instances of *nonterminals symbols*. \n",
    "\n",
    "We will create classes to represent terminals and nonterminals separetely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a class for a general *Symbol*, this class does not do much, but it gives terminals and nonterminals a common interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symbol:\n",
    "    \"\"\"\n",
    "    A symbol in a grammar, this is an abstract class that does not do \n",
    "    anything other than to establish the minimum interface of terminals \n",
    "    and nonterminals.\n",
    "\n",
    "    A symbol is an object much like a python string, but we don't need \n",
    "    operations such as concatenation and other string operations. \n",
    "    We just need to be able to compare symbols and check their identity. \n",
    "\n",
    "    Some of the methods below must be implemented in classes that \n",
    "    inherit from Symbol. \n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"This method allows us to inspect the identity of the symbol\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Children classes must implement this method')\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"\n",
    "        This returns a hash value, python needs this in order to \n",
    "        use a Symbol in hash-based containers such as sets and dictionaries\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Children classes must implement this method')\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"This returns whether or not two symbols are the same\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'Children classes must implement this method')\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        \"\"\"\n",
    "        This is necessary so that python can sort symbols lexicographically.\n",
    "\n",
    "        We just delegate this comparison function to python, \n",
    "        since it knows how to compare strings, \n",
    "        we use the result of __str__ applied to each symbol.\n",
    "        \"\"\"\n",
    "        return str(self) < str(other)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"We just make __repr__ and __str__ return the same\"\"\"\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a **Terminal** class, symbols of this type are simply words in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Terminal(Symbol):\n",
    "    \"\"\"\n",
    "    A type of Symbol that acts like a constant. \n",
    "\n",
    "    A terminal is simply a container for a python string which represents \n",
    "    the actual symbol. For example, a terminal symbol 'cat' simply \n",
    "    contains the python string 'cat' in it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word: str):\n",
    "        \"\"\"\n",
    "        word: a python string representing a word\n",
    "        \"\"\"\n",
    "        self._word = word\n",
    "\n",
    "    @property\n",
    "    def word(self):\n",
    "        return self._word\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        We print terminals around single quotes, this gives a nice visual clue.\n",
    "\n",
    "        If you need the underlying word (as a python string) and without \n",
    "        the single quotes around it, then you need to use the property \n",
    "        Terminal.word.\n",
    "        \"\"\"\n",
    "        return \"'%s'\" % self._word\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\" This is necessary in order for python to be able to use Terminal \n",
    "            objects in sets and dictionaries\"\"\"\n",
    "        # Python knows how to compute hash values for basic types \n",
    "        # (str, tuple, etc.), so we delegate to python this job.\n",
    "        # This is a good idea since we don't really know how to write \n",
    "        # good hash functions and the team of python developers must \n",
    "        # have put quite some work into it.\n",
    "        return hash(self._word)\n",
    "\n",
    "    def __eq__(self, other: Symbol):\n",
    "        \"\"\" This is necessary if order for python to be able to compare \n",
    "            terminals symbols\"\"\"\n",
    "        # Two symbols are the same if they are both terminals\n",
    "        #  and the words they stand for are also the same\n",
    "        return isinstance(other, Terminal) and other._word == self._word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a **Nonterminal** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nonterminal(Symbol):\n",
    "    \"\"\"\n",
    "    A type of Symbol that acts like a variable. \n",
    "\n",
    "    A nonterminal is simply a container for a python string which represents the actual symbol. \n",
    "    For example, a nonterminal symbol [NP] simply contains the python string 'NP' in it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, category: str):\n",
    "        \"\"\"\n",
    "        category: a python string representing a POS or phrasal category\n",
    "        \"\"\"\n",
    "        self._category = category\n",
    "\n",
    "    @property\n",
    "    def category(self):\n",
    "        return self._category\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        We print nonterminals wrapped around squared brackets to make a visual clue.\n",
    "        If you need the underlying category (as a python string) without the brackets around it, \n",
    "         then you need to use the property Nonterminal.category.\n",
    "        \"\"\"\n",
    "        return \"[%s]\" % self._category\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"Python can only hash objects that properly implement this method\"\"\"\n",
    "        # Again we delegate to python the job of computing a hash value for our category\n",
    "        return hash(self._category)\n",
    "\n",
    "    def __eq__(self, other: Symbol):\n",
    "        \"\"\"Python needs this in order to compare nonterminal symbols\"\"\"\n",
    "        # Two symbols are the same if they are both nonterminals\n",
    "        #  and their categories are the same\n",
    "        return isinstance(other, Nonterminal) and other._category == self._category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use our classes and see what we got so far. \n",
    "\n",
    "Here is how we use *Terminal* objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terminal('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we implemented `__eq__` and therefore we can test equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terminal('cat') == Terminal('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terminal('cat') == Terminal('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also test inequality because by default python assumes this to be the result of negating what `__eq__` returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terminal('cat') != Terminal('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can do all of the above with nonterminals as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nonterminal('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nonterminal('S') == Nonterminal('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nonterminal('S') == Nonterminal('VP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nonterminal('S') != Nonterminal('VP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've implemented `__eq__` and `__hash__` our symbols are *hashable* objects. This means that python will do the correct thing whenener we try to use these symbols in hash-based data structures such as sets and dictionaries.\n",
    "\n",
    "Let's play a bit with sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{Terminal('cat'), Terminal('cat'), Terminal('dog')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{Nonterminal('S'), Nonterminal('NP'), Nonterminal('VP'),  Nonterminal('VP')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make a small vocabulary of terminals\n",
    "mini_t_vocab = {Terminal('cat'), Terminal('bird'), Terminal('dog')}\n",
    "# and map terminals to an enumeration of the set\n",
    "{x: i for i, x in enumerate(mini_t_vocab, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also play a bit with dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make a small vocabulary of nonterminals\n",
    "mini_nt_vocab = {Nonterminal('S'), Nonterminal('NP'), Nonterminal('VP')}\n",
    "# and map nonterminals to an enumeration of the set\n",
    "{x: i for i, x in enumerate(mini_nt_vocab, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously our objects can also be added to python lists, but for that we need no special treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Nonterminal('NP'), Terminal('and'), Nonterminal('NP')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"rules\"> Rules\n",
    "\n",
    "We are now ready to define context-free **rules** or context-free *productions*.\n",
    "\n",
    "A context-free rule is an object of the kind $\\text{X} \\rightarrow \\beta $ where\n",
    "* $\\text{X}$ is a nonterminal symbol\n",
    "* and $\\beta$ is a sequence of terminals and nonterminals\n",
    "\n",
    "In python we can easily represent a rule as a container for a left-hand side (LHS) nonterminal and a right-hand side (RHS) sequence. Check the class below (in particular its documentation).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-1\" style=\"color:red\">**Exercise 6-1**</a> **[1 point]** Implement checks to validate that a rule is well-formed at construction time. In other words, read the documentation of the class `Rule` below and complete its constructor `__init__`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Rule:\n",
    "    \"\"\"\n",
    "    A Rule is just a container, in particular, a pair. \n",
    "\n",
    "    It stores a LHS nonterminal and a RHS sequence of symbols.\n",
    "\n",
    "    In general, RHS could be empty, but we will not implement such grammars.\n",
    "    We will restrict the RHS to containing at least one symbol.    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lhs: Nonterminal, rhs: list):\n",
    "        \"\"\"\n",
    "        Constructs a Rule: LHS -> RHS.\n",
    "\n",
    "        We must validate that rules are well formed, that is, the LHS symbol is indeed a Nonterminal, \n",
    "            the RHS is *not* empty and only contains Symbol (i.e. Terminal or Nonterminal) objects.\n",
    "\n",
    "        lhs: the LHS nonterminal\n",
    "        rhs: a sequence of RHS symbols\n",
    "        \"\"\"\n",
    "        if (not isinstance(lhs, Nonterminal)):\n",
    "            raise ValueError('lhs is not a nonterminal')\n",
    "        for x in rhs:\n",
    "            if (not isinstance(x, Symbol)):\n",
    "                raise ValueError('rhs contains a non-symbol')\n",
    "\n",
    "        self._lhs = lhs\n",
    "        self._rhs = tuple(rhs)\n",
    "\n",
    "    def __eq__(self, other: 'Rule'):\n",
    "        \"\"\"Two rules are the same if they have the same LHS and the same RHS\"\"\"\n",
    "        return self._lhs == other._lhs and self._rhs == other._rhs\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"The hash value of the Rule is the hash value of the pair (LHS, RHS)\"\"\"\n",
    "        # Once more we delegate the computation of the hash value to python\n",
    "        return hash((self._lhs, self._rhs))\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"We print rules using the notation LHS -> RHS\"\"\"\n",
    "        return '%s -> %s' % (self._lhs, ' '.join(str(sym) for sym in self._rhs))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    @property\n",
    "    def lhs(self):\n",
    "        \"\"\"Returns the LHS nonterminal\"\"\"\n",
    "        return self._lhs\n",
    "\n",
    "    @property\n",
    "    def rhs(self):\n",
    "        \"\"\"Returns the RHS sequence\"\"\"\n",
    "        return self._rhs\n",
    "\n",
    "    @property\n",
    "    def arity(self):\n",
    "        \"\"\"Returns the arity (length of the RHS sequence)\"\"\"\n",
    "        return len(self._rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use *Rule* objects, let's first create a few symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Nonterminal('S')\n",
    "NP = Nonterminal('NP')\n",
    "VP = Nonterminal('VP')\n",
    "cats = Terminal('cats')\n",
    "run = Terminal('run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we can then combine into rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule(S, [NP, NP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule(NP, [cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule(VP, [run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule(S, [NP, VP, Terminal('and'), VP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rule(S, [NP, NP, Terminal('.')])\n",
    "print('Rule', r)\n",
    "print('LHS', r.lhs)\n",
    "print('RHS', r.rhs)\n",
    "print('Arity', r.arity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a context-free grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"grammar\"> Grammar\n",
    "\n",
    "A context free grammar is again just a container, this time for context-free rules. \n",
    "\n",
    "A grammar is formally specified by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathfrak G = \\langle \\Sigma, \\mathcal V, \\text{S}, \\mathcal R \\rangle \n",
    "\\end{equation}\n",
    "\n",
    "* a *finite set* of **terminals** which we denote $\\Sigma$\n",
    "* a *finite set* of **nonterminals** which we denote $\\mathcal V$\n",
    "* a distinguished nonterminal $\\text{S}$ called the *start symbol*\n",
    "* a *finite set* of context-free **rules** which we denote $\\mathcal R$\n",
    "    * each rule is of the form $v \\rightarrow \\beta$ for some $v \\in \\mathcal V$ and $\\beta \\in (\\Sigma \\cup \\mathcal V)^a$\n",
    "    * and $a$ is the **arity** of the grammar, that is, the size of the longest RHS sequence in a rule of the grammar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-2\" style=\"color:red\">**Exercise 6-2**</a> **[1 points]** If we have $|\\Sigma|$ terminals, $|\\mathcal V|$ nonterminals, and $a$ is the arity of the grammar, how many rules can there be asymptotically (use big-o-notation)? Explain the result rather than simply stating the solution.\n",
    "\n",
    "$O(\\mathcal{V} \\times \\mathcal{V}^a)$ or $O(\\mathcal{V} \\times \\Sigma^a)$\n",
    "\n",
    "depending on if $\\mathcal{V} > \\Sigma$\n",
    "\n",
    "Explanation: we have at least one non terminal on the left. Then on the right, we have as many possibilities as the arity of the grammar multiplied by either the amount of terminals or non-terminals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, we will implement a grammar as a container that behaves like a mixture of list, set, and dict. For example, it behaves like a list because we are able to iterate through the rules, it behaves like a set because we can check whether a rule belongs to the grammar, and it also behaves like a dictionary mapping a LHS Nonterminal to rules that have that nonterminal as their LHS symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"\n",
    "    A CFG is a container for rules.\n",
    "\n",
    "    Internally we maintain a few sets:\n",
    "    * _terminals: the set of terminal symbols\n",
    "    * _nonterminal: the set of all nonterminal symbols     \n",
    "    * _preterminals: a subset of nonterminal symbols that correspond to POS categories\n",
    "\n",
    "    We also maintain a set of rules: _rules\n",
    "\n",
    "    We also maintain a dictionary whose keys are Nonterminal LHS symbols, and whose values are lists of rules.\n",
    "    Thus, if X is a symbol, _rules_by_lhs[X] should return the list of rules that share that symbol as their\n",
    "        LHS nonterminal.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_symbol: Nonterminal):\n",
    "        if not isinstance(start_symbol, Nonterminal):\n",
    "            raise ValueError('The start symbol must be a nonterminal')\n",
    "        self._start = start_symbol\n",
    "        # this should contain all rules of the grammar\n",
    "        self._rules = set()\n",
    "        # this should map a LHS symbol to a list of context-free rules rewriting that symbol\n",
    "        self._rules_by_lhs = defaultdict(list)\n",
    "        # this should contain all terminals\n",
    "        self._terminals = set()\n",
    "        # this should contain all nonterminals (including start symbol and preterminals)\n",
    "        self._nonterminals = set()\n",
    "        # this should contain only pre-terminals (that is, POS categories)\n",
    "        self._preterminals = set()\n",
    "        # length of the longest RHS\n",
    "        self._arity = 0\n",
    "\n",
    "    def add(self, rule: Rule):\n",
    "        \"\"\"\n",
    "        Add a rule to the ruleset, unless the rule is already known.\n",
    "        This method also updates the sets of symbols with the symbols in this rule.\n",
    "        \"\"\"\n",
    "        if rule in self._rules:  # we do not add repeated rules\n",
    "            return\n",
    "        # add rule to ruleset\n",
    "        self._rules.add(rule)\n",
    "        # also maps it for convenience\n",
    "        self._rules_by_lhs[rule.lhs].append(rule)\n",
    "        # the rule's LHS is now part of the nonterminal set\n",
    "        self._nonterminals.add(rule.lhs)\n",
    "        # and we should also add all other symbols in the rule\n",
    "        for sym in rule.rhs:\n",
    "            if isinstance(sym, Terminal):  # terminals\n",
    "                self._terminals.add(sym)\n",
    "            else:  # nonterminals\n",
    "                self._nonterminals.add(sym)\n",
    "        # a preterminal rule has arity 1 and rewrites to a terminal\n",
    "        if rule.arity == 1 and isinstance(rule.rhs[0], Terminal):\n",
    "            self._preterminals.add(rule.lhs)\n",
    "        # here we update the arity of the grammar\n",
    "        if rule.arity > self._arity:\n",
    "            self._arity = rule.arity\n",
    "\n",
    "    def update(self, rules):\n",
    "        \"\"\"Adds a collection of rules to the grammar\"\"\"\n",
    "        for rule in rules:\n",
    "            self.add(rule)\n",
    "\n",
    "    @property\n",
    "    def start(self):\n",
    "        return self._start\n",
    "\n",
    "    @property\n",
    "    def nonterminals(self):\n",
    "        return self._nonterminals\n",
    "\n",
    "    @property\n",
    "    def preterminals(self):\n",
    "        return self._preterminals\n",
    "\n",
    "    @property\n",
    "    def terminals(self):\n",
    "        return self._terminals\n",
    "\n",
    "    @property\n",
    "    def arity(self):\n",
    "        \"\"\"Returns the arity of the longest rule\"\"\"\n",
    "        return self._arity\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The size of the grammar in number of rules\"\"\"\n",
    "        return len(self._rules)\n",
    "\n",
    "    def __getitem__(self, lhs: Nonterminal):\n",
    "        \"\"\"Returns rules for a certain LHS symbol\"\"\"\n",
    "        return self._rules_by_lhs.get(lhs, frozenset())\n",
    "\n",
    "    def get(self, lhs: Nonterminal, default=frozenset()):\n",
    "        \"\"\"Return rules whose LHS is the given symbol\"\"\"\n",
    "        return self._rules_by_lhs.get(lhs, frozenset())\n",
    "\n",
    "    def can_rewrite(self, lhs: Nonterminal):\n",
    "        \"\"\"\n",
    "        Whether a given nonterminal can be rewritten. In other words, do we know a rule whose LHS is this\n",
    "         symbol?\n",
    "        \"\"\"\n",
    "        return lhs in self._rules_by_lhs\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterator over rules (in arbitrary order)\"\"\"\n",
    "        return iter(self._rules)\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"Iterator over pairs of the kind (LHS, rules rewriting LHS)\"\"\"\n",
    "        return self._rules_by_lhs.items()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Converts all rules to string\"\"\"\n",
    "        lines = []\n",
    "        # First rules for the start symbol\n",
    "        for rule in self[self._start]:\n",
    "            lines.append(str(rule))\n",
    "        # Then other rules (except pre-terminal ones)\n",
    "        for lhs, rules in sorted(self.items(), key=lambda pair: pair[0]):\n",
    "            if lhs == self._start or lhs in self._preterminals:\n",
    "                continue\n",
    "            for rule in rules:\n",
    "                lines.append(str(rule))\n",
    "        # And finally the pre-terminal rules\n",
    "        for pos in sorted(self._preterminals):\n",
    "            for rule in self[pos]:\n",
    "                lines.append(str(rule))\n",
    "        # Now we concatenate them all\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'CFG: start=%s n_terminals=%d n_nonterminals=%d n_rules=%d' % (\n",
    "            self._start, len(self._terminals), len(\n",
    "                self._nonterminals), len(self._rules)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_grammar(np_cc_np=True, vp_cc_vp=True):\n",
    "    # Some symbols\n",
    "    S = Nonterminal('S')\n",
    "    NP = Nonterminal('NP')\n",
    "    N = Nonterminal('N')\n",
    "    VP = Nonterminal('VP')\n",
    "    V = Nonterminal('V')\n",
    "    # Grammar\n",
    "    G = CFG(S)\n",
    "    # Phrasal rules\n",
    "    G.add(Rule(S, [NP, VP]))\n",
    "    G.add(Rule(NP, [N]))\n",
    "    G.add(Rule(VP, [V]))\n",
    "    # Preterminal rules\n",
    "    G.add(Rule(N, [Terminal('cats')]))\n",
    "    G.add(Rule(N, [Terminal('dogs')]))\n",
    "    G.add(Rule(N, [Terminal('birds')]))\n",
    "    G.add(Rule(V, [Terminal('run')]))\n",
    "    G.add(Rule(V, [Terminal('bark')]))\n",
    "    G.add(Rule(V, [Terminal('meow')]))\n",
    "    G.add(Rule(V, [Terminal('chirp')]))\n",
    "    # Making the grammar more complex\n",
    "    if np_cc_np or vp_cc_vp:\n",
    "        CC = Nonterminal('CC')\n",
    "        G.add(Rule(CC, [Terminal('and')]))\n",
    "        if np_cc_np:\n",
    "            G.add(Rule(NP, [NP, CC, NP]))\n",
    "        if vp_cc_vp:\n",
    "            G.add(Rule(VP, [VP, CC, VP]))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = get_toy_grammar()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of how to use the CFG object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Nonterminal('N')\n",
    "for rule in G[N]:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in G.preterminals:\n",
    "    print('This is a POS category: %s' % pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Nonterminal('S')\n",
    "print('We can rewrite the symbol %s: %s' % (S, G.can_rewrite(S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Nonterminal('T')\n",
    "print('We can rewrite the symbol %s: %s' % (T, G.can_rewrite(T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We known %d rules to rewrite the symbol %s' % (len(G[N]), N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"generation\"> Generation\n",
    "\n",
    "\n",
    "We can use CFG grammars to generate strings, the basic idea is really simple, we start from the grammars start symbol\n",
    "\\begin{align}\n",
    "(1) \\quad \\langle \\text{S} \\rangle\n",
    "\\end{align}\n",
    "then we randomly select a rule that rewrites it. Suppose that we have only 1 rule available `S -> NP VP`, then the only thing we can do at this point is to expand `S`\n",
    "\\begin{align}\n",
    "(1) &\\quad \\langle \\text{S} \\rangle  \\\\\n",
    "(2) &\\quad \\langle \\text{NP} \\, \\text{VP} \\rangle   \\\\\n",
    "\\end{align}\n",
    "\n",
    "We then recursively repeat this procedure for the leftmost nonterminal. Suppose that we can rewrite `NP` as `N`, or `D N`, or `NP NP`. Then we would have to select one of those 3 options. Let's assume for now that we pick one of them uniformly at random. For example, suppose we choose `N`, then the derivation proceeds as follows:\n",
    "\\begin{align}\n",
    "(1) &\\quad \\langle \\text{S} \\rangle \\\\\n",
    "(2) &\\quad \\langle \\text{NP} \\, \\text{VP} \\rangle \\\\\n",
    "(3) &\\quad \\langle \\text{N} \\, \\text{VP} \\rangle \\\\\n",
    "\\end{align}\n",
    "we recursively repeat this procedure\n",
    "\\begin{align}\n",
    "(1) &\\quad \\langle \\text{S} \\rangle \\\\\n",
    "(2) &\\quad \\langle \\text{NP} \\, \\text{VP} \\rangle \\\\\n",
    "(3) &\\quad \\langle \\text{N} \\, \\text{VP} \\rangle \\\\\n",
    "(4) &\\quad \\langle \\text{cats} \\, \\text{VP} \\rangle \\\\\n",
    "\\end{align}\n",
    "until we are left with terminal symbols only. For example\n",
    "\\begin{align}\n",
    "(1) &\\quad \\langle \\text{S} \\rangle \\\\\n",
    "(2) &\\quad \\langle \\text{NP} \\, \\text{VP} \\rangle \\\\\n",
    "(3) &\\quad \\langle \\text{N} \\, \\text{VP} \\rangle \\\\\n",
    "(4) &\\quad \\langle \\text{cats} \\, \\text{VP} \\rangle \\\\\n",
    "(5) &\\quad \\langle \\text{cats} \\, \\text{V} \\rangle \\\\\n",
    "(6) &\\quad \\langle \\text{cats} \\, \\text{meow} \\rangle \\\\\n",
    "\\end{align}\n",
    "\n",
    "Then the sequence of rule applications (in *this order*) \n",
    "\\begin{align}\n",
    "\\langle \\text{S} \\rightarrow \\text{NP} \\, \\text{VP} ,\\\\\n",
    "\\text{NP} \\rightarrow \\text{N} , \\\\\n",
    "\\text{N} \\rightarrow \\text{cats} , \\\\\n",
    "\\text{VP} \\rightarrow \\text{V} , \\\\\n",
    "\\text{V} \\rightarrow \\text{meow} \\rangle\n",
    "\\end{align}\n",
    "is our **derivation**, and the final sequence of words\n",
    "\\begin{equation}\n",
    "\\langle \\text{cats}, \\, \\text{meow} \\rangle\n",
    "\\end{equation}\n",
    "is the **yield**.\n",
    "\n",
    "Below you will implement this **depth-first traversal**, that is, a top-down and left-to-right recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-3\" style=\"color:red\">**Exercise 6-3**</a> **[3 points]** Implement a recursive algorithm that randomly generates derivations and their yields from the start symbol of the grammar. For now, use a uniform distribution to select which rule rewrites a symbol whenever multiple rules are available.\n",
    "\n",
    "* Guidelines: complete the function below (note that you can write additional functions which you can from within the function below, if you want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def choose_uniformly(nb_objects):\n",
    "    \"\"\"\n",
    "    This is a helper function that reminds you how to choose 1 of k objects uniformly.\n",
    "\n",
    "    :param nb_objects: total number of objects to choose from\n",
    "    :returns: an index from 0 to nb_objects - 1 (because we assumed 0-based indexing)\n",
    "    \"\"\"\n",
    "    # and make a uniform distribution over them\n",
    "    uniform_dist = np.ones(nb_objects) / nb_objects\n",
    "    # we then select one of the available rules uniformly at random\n",
    "    i = np.random.choice(nb_objects, p=uniform_dist)\n",
    "    return i\n",
    "\n",
    "\n",
    "def generate_uniform_sample(cfg: CFG):\n",
    "    \"\"\"\n",
    "    Given a CFG,returns a derivation (and its yield) sampled top-down uniformly at random.\n",
    "\n",
    "    Recall that a derivation is a depth-first traversal, that is, \n",
    "        we always rewrite the leftmost nonterminal symbol first.\n",
    "\n",
    "    :param cfg: a CFG grammar    \n",
    "    :returns: derivation (sequence of rules), yield (sequence of words)\n",
    "    \"\"\"\n",
    "    yield_return = []\n",
    "    S = CFG._start\n",
    "    for rule in CFG[S]:\n",
    "        print(rule)\n",
    "    pass\n",
    "\n",
    "\n",
    "G2 = get_toy_grammar(np_cc_np=True, vp_cc_vp=False)\n",
    "generate_uniform_sample(G2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a bit from this grammar.\n",
    "\n",
    "Note: as we are sampling uniformly, sometimes the derivations might be quite long, to reduce that effect, let's play with a simpler grammar which allows noun phrases to grow recursively, but not verb phrases. \n",
    "\n",
    "\n",
    "```python\n",
    "G2 = get_toy_grammar(np_cc_np=True, vp_cc_vp=False)\n",
    "generate_uniform_sample(G2)\n",
    "# you will get something like the following (but of course this is a random example)\n",
    "# (([S] -> [NP] [VP], [NP] -> [N], [N] -> 'cats', [VP] -> [V], [V] -> 'bark'), ('cats', 'bark'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we are sampling uniformly at each step, later we will give CFGs a probabilistic treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognisers\n",
    "\n",
    "Now we want to design *recognisers*, that is, algorithms that can decide whether or not a string belongs to the language of the grammar.\n",
    "\n",
    "The *language of the grammar* $L(\\mathfrak G)$ is the set of all terminal strings that can be derived from the grammar's start symbol by application of grammar rules. Formally, \n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathfrak G) = \\{\\omega \\in \\Sigma^*: \\text{S} \\overset{*}{\\Rightarrow} \\omega\\}\n",
    "\\end{equation}\n",
    "\n",
    "We read this as:\n",
    "* $\\omega$ is a string of terminals, which we know because $\\omega$ lives in the set of all strings, the set of all strings is denoted $\\Sigma^*$ and $\\omega \\in \\Sigma^*$ means that $\\omega$ is one string in that set\n",
    "* then the language $L(\\mathfrak G)$ is a set of strings such that (denoted by $:$) each and every string in the language can be derived from the grammar's start symbol $\\text{S}$\n",
    "\n",
    "We can design recognisers that work top-down as well as bottom-up, that is, from start symbol to the strings or vice-versa. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"shiftreduce\"> Shift-Reduce\n",
    "\n",
    "Shift-reduce is a bottom-up recogniser. The following deductive system completely specifies it:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Item} &\\qquad [\\alpha \\, \\bullet, i] \\\\\n",
    "\\text{Goal} &\\qquad [\\text{S} \\, \\bullet, n] \\\\\n",
    "\\text{Axiom} &\\qquad [\\bullet, 0] \\\\\n",
    "\\text{Shift} &\\qquad \\frac{[\\alpha \\, \\bullet, i]}{[\\alpha \\, x_{i+1}, i + 1]} \\\\\n",
    "\\text{Reduce} & \\qquad \\frac{[\\alpha \\, \\beta \\, \\bullet, i]}{[\\alpha \\, X, i]} \\quad X \\rightarrow \\beta \\in \\mathcal R\n",
    "\\end{align}\n",
    "\n",
    "Let's understand the *item* form\n",
    "* we have a string $\\alpha$ which is a (possibly empty) sequence of terminals and nonterminal symbols\n",
    "* this string is typically known as a *stack*, that is the case because we typically use stacks to implement it\n",
    "* and we have an integer $i \\in [0, n]$ that denotes how many words of the input sentence we have parsed so far (from left-to-right)\n",
    "* when $i=0$ we haven't yet parsed anything\n",
    "* we interpret an item $[\\alpha \\bullet, i]$ as: *we have parsed $i$ words and we have justified them with the symbols in the stack $\\alpha$*\n",
    "* the dot $\\bullet$ in the item does not really have a function, it just reminds us that the algorithm expands the string inside of the item from left-to-right\n",
    "\n",
    "Let's understand the *goal*\n",
    "* the goal is simply to parse the complete sentence, that is, parse $n$ input words\n",
    "* and at the end of the algorithm we must have only the grammar's start symbol $\\text{S}$ in the stack\n",
    "\n",
    "Let's understand the *axioms*\n",
    "* there's only a single axiom item, namely, an item that states that we have not yet parsed anything (and thus we have an empty stack)\n",
    "\n",
    "Let's understand *shift*\n",
    "* this inference rule simply pushes a terminal (a word) into the stack\n",
    "* this means that the position $i$ advances by 1\n",
    "\n",
    "Let's understand *reduce*\n",
    "* this rule checks the top of the stack for a string that matches the RHS sequence of one of the grammar's rules\n",
    "* if we find such a sequence $\\beta$ and such a rule, e.g. $X \\rightarrow \\beta$, then we replace $\\beta$ by $X$ in the top of the stack\n",
    "* note that this rule does not advance over the string ($i$ does not change), it only *reduces* the size of the stack\n",
    "\n",
    "In python, we implement shift-reduce by programming the item, the rules (i.e. axiom, shift, reduce), and the deduction loop. We use an agenda of *active* items in order to explore the space of valid inferences. The logic is simple, we must process every item exactly once and we must give every rule a chance to infer new items from existing ones. \n",
    "\n",
    "We implement this parsing strategy below for you. Study it carefuly as it explains the general design necessary to implement deductive systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class ShiftReduce:\n",
    "    \"\"\"\n",
    "    A bottom-up recogniser for epsilon-free CFGs.\n",
    "\n",
    "    Here an Item is a pair (sequence, position) where\n",
    "    * sequence is a tuple of terminals and nonterminals\n",
    "    * position is a positive integer \n",
    "\n",
    "    The algorithm is based on the deductive system specified above and we use one auxiliary data-structure\n",
    "     to map from RHS sequences to sets of LHS. This kind of reverses the CFG internal dictionary of rules.\n",
    "     We use this in order to make the check in the REDUCE rule a little easier to perform (see below).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: CFG):\n",
    "        self._cfg = cfg\n",
    "        # this reverses the CFG internal rule map\n",
    "        # here we map from RHS to LHS\n",
    "        # since there are many rules (with different LHS) that may produce the same RHS\n",
    "        # we use a defaultdict of sets\n",
    "        self._rhs_to_lhs = defaultdict(set)\n",
    "        for r in self._cfg:\n",
    "            self._rhs_to_lhs[r.rhs].add(r.lhs)\n",
    "\n",
    "    def make_item(self, stack, position: int):\n",
    "        \"\"\"\n",
    "        Returns a pair (stack, position)\n",
    "        \"\"\"\n",
    "        return (stack, position)\n",
    "\n",
    "    def axioms(self):\n",
    "        \"\"\"\n",
    "        Returns a set of axiomatic items.\n",
    "         In fact, we only have a single axiom: the empty-stack, 0-position item.\n",
    "        \"\"\"\n",
    "        return {self.make_item(tuple(), 0)}\n",
    "\n",
    "    def shift(self, item, sentence):\n",
    "        \"\"\"\n",
    "        Returns a set of items according to the SHIFT rule.\n",
    "         This is in fact either no item or a single item.\n",
    "        \"\"\"\n",
    "        stack, position = item\n",
    "        items = set()\n",
    "        # shift\n",
    "        if position < len(sentence):\n",
    "            # expand the stack by pushing sentence[position] onto its top\n",
    "            # we also advance the position variable\n",
    "            items.add(self.make_item(\n",
    "                stack + (sentence[position],), position + 1))\n",
    "        return items\n",
    "\n",
    "    def reduce(self, item):\n",
    "        \"\"\"\n",
    "        Returns a set of new items according to the REDUCE rule.\n",
    "        \"\"\"\n",
    "        # This is the current item\n",
    "        stack, position = item\n",
    "        new_items = set()\n",
    "        for a in range(1, len(stack) + 1):  # we will inspect the top symbols in the stack\n",
    "            # here we get the last a symbols\n",
    "            suffix = stack[-a:]\n",
    "            # and here everything else\n",
    "            prefix = stack[: -a]\n",
    "            # if there are rules that can cover this suffix\n",
    "            for lhs in self._rhs_to_lhs.get(tuple(suffix), set()):\n",
    "                # we replace the top of the stack with the LHS of those rules\n",
    "                # and we do not touch the position variable\n",
    "                new_items.add(self.make_item(tuple(prefix) + (lhs,), position))\n",
    "            # there is no point in trying sequences longer than the grammar's arity\n",
    "            if a >= self._cfg.arity:\n",
    "                break\n",
    "        return new_items\n",
    "\n",
    "    def recognise(self, sentence):\n",
    "        \"\"\"\n",
    "        Test whether a sentence belong to the language of the grammar.\n",
    "\n",
    "        The general strategy with deductive systems is always the same:\n",
    "\n",
    "            - we need to manage `active items`, some form of agenda of items to be processed\n",
    "            - we also need some form or mechanism to track items that have already been discovered\n",
    "            - then we exhaust the agenda with something like `while agenda`\n",
    "            - we check whether we have derived the goal item, and if so, we return True\n",
    "            - if we have not, we continue by trying to shift and reduce, which causes new items to become active\n",
    "\n",
    "        :param sentence: a sequence of Terminal symbols\n",
    "        :returns: True if S =>* sentence, False otherwise\n",
    "        \"\"\"\n",
    "        # our agenda is a python double ended queue\n",
    "        #  but it could well be a list, we are using deque so you learn about it\n",
    "        agenda = deque(self.axioms())\n",
    "\n",
    "        # we need to maintain a set of already discovered items,\n",
    "        #  we use this to prevent adding the same item multiple times to the agenda\n",
    "        #  as the agenda is not a hash-based object, searching through it would take too long\n",
    "        #  (linear time on the agenda size)\n",
    "        #  instead we use a set which has average access time O(1)\n",
    "        discovered = set(agenda)\n",
    "\n",
    "        # We iterate for as long as there are active items in the agenda\n",
    "        while agenda:\n",
    "            # we chose to pop from the agenda's end, which gives it a stack treatment\n",
    "            # this is not necessarily the only alternative\n",
    "            # we could have done agenda.popleft() which would give it a queue treatment\n",
    "            # the stack treatment typically works a bit faster ;)\n",
    "            item = agenda.pop()\n",
    "            # recall the item form [\\alpha, i]\n",
    "            stack, position = item\n",
    "            # GOAL: check whether this is the GOAL item: [S, n]\n",
    "            if position == len(sentence) and len(stack) == 1 and stack[-1] == self._cfg.start:\n",
    "                # we accept the sentence\n",
    "                return True\n",
    "            # SHIFT\n",
    "            for new_item in self.shift(item, sentence):\n",
    "                if new_item not in discovered:  # we never process an item twice\n",
    "                    discovered.add(new_item)\n",
    "                    agenda.append(new_item)\n",
    "            # REDUCE\n",
    "            for new_item in self.reduce(item):\n",
    "                if new_item not in discovered:  # we never process an item twice\n",
    "                    discovered.add(new_item)\n",
    "                    agenda.append(new_item)\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogniser = ShiftReduce(get_toy_grammar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogniser.recognise([Terminal(\n",
    "    w) for w in 'cats and dogs and birds run and meow and bark and chirp'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogniser.recognise([Terminal(\n",
    "    w) for w in 'cats and dogs and birds run and jump and bark and chirp'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"cky\"> CKY\n",
    "\n",
    "If we know our grammars obey to a certain form, such as *Chomsky Normal Form* (CNF) we can design efficient recognisers. One such recogniser is the CKY algorithm for CNF grammars.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-4\" style=\"color:red\">**Exercise 6-4**</a> **[2 points]** Describe a CNF grammar (do not simply state the conditions, explain them in English).\n",
    "\n",
    "the rules of a CNF grammar need to have one of these three forms:\n",
    "\n",
    "A -> BC (where A, B and C are non-terminals)\n",
    "\n",
    "A -> a (where 'A' is a non-terminal and 'a' is a terminal)\n",
    "\n",
    "S -> $\\epsilon$ (where S is the start symbol and $\\epsilon$ is an empty string\n",
    "\n",
    "furthermore, A != S, B != S, and (S -> $\\epsilon$) can only be used if $\\epsilon$ is in the language of the grammar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deductive system for CKY**\n",
    "\n",
    "The following deductive system specifies the CKY parsing strategy\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Item} &\\qquad [i, X, j] \\\\\n",
    "\\text{Goal} &\\qquad [0, \\text{S}, n] \\\\\n",
    "\\text{Axiom} &\\qquad [i, X, i + 1] \\quad X \\rightarrow x_i \\in \\mathcal R\\\\\n",
    "\\text{Merge} & \\qquad \\frac{[i, A, k] \\, [k, B, j]}{[i, C, j]} \\quad C \\rightarrow A \\, B \\in \\mathcal R\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Now you will implement the CKY recogniser:\n",
    "* we expect you to use as items a tuple such as `(start, nonterminal, end)`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-5\" style=\"color:red\">**Exercise 6-5**</a> **[4 points]** Explain the following \n",
    "\n",
    "* **[1 point]** CKY items\n",
    "* **[1 point]** CKY goal\n",
    "* **[1 point]** CKY axioms\n",
    "* **[1 point]** CKY merge rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "<a name=\"ex6-6\" style=\"color:red\">**Exercise 6-6**</a> **[9 points]** Here you will implement a CKY recogniser.\n",
    "\n",
    "Start by carefully checking the documentation and relating each method to a part of the deductive system, then carefully study the form of the CKY item (which is implemented in the method `make_item`). Then solve the exercises below:\n",
    "\n",
    "* **[1 point]** Implement the checks for CNF grammars in the constructor\n",
    "* **[1 point]** Implement the GOAL check in the method `is_goal` (check the documentation)\n",
    "* **[1 point]** Implement the axioms of the system in the method `axioms` (check the documentation)\n",
    "* **[2 points]** Implement the rule *merge* in the method `merge` (check the documentation)\n",
    "* **[3 points]** Implement the main loop in `recognise`\n",
    "* **[1 point]** Show that you get the correct answers for the grammar in `get_toy_cnf_grammar` below and the following sentences:\n",
    "    * `cats and dogs run`\n",
    "    * `cats and dogs jump`\n",
    "    * try some additional examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class CKYRecogniser:\n",
    "    \"\"\"\n",
    "    A CKY recogniser for CFGs in CNF which is implemented as a deductive system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: CFG):\n",
    "        \"\"\"\n",
    "        This constructs the recogniser for a certain grammar.\n",
    "\n",
    "        You may use this as an opportunity to precompute certain quantities or cache certain objects\n",
    "         if that helps you.\n",
    "\n",
    "        You should make sure the input is indeed in CNF.\n",
    "\n",
    "        :param cfg: a CFG in Chomsky Normal Form (CNF)\n",
    "        \"\"\"\n",
    "        # BEGIN OF SOLUTION: CNF checks\n",
    "        # ...\n",
    "        # END OF SOLUTION: CNF checks\n",
    "\n",
    "        self._cfg = cfg\n",
    "        # It is convenient to map from RHS to rules\n",
    "        self._rhs_to_rules = defaultdict(set)\n",
    "        for r in cfg:\n",
    "            self._rhs_to_rules[r.rhs].add(r)\n",
    "\n",
    "    def make_item(self, rule: Rule, start: int, end: int):\n",
    "        \"\"\"\n",
    "        We can create items based on a rule which justifies a span of the sentence.\n",
    "        :param rule: a Rule object\n",
    "        :param start: an integer from 0 to n\n",
    "        :param end: an integer from 0 to n (where end >= start)\n",
    "        :returns: a CKY item represented as a tuple (start, rule.lhs, end)\n",
    "        \"\"\"\n",
    "        return (start, rule.lhs, end)\n",
    "\n",
    "    def get_category(self, item):\n",
    "        \"\"\"Returns the phrase category of the CKY item\"\"\"\n",
    "        return item[1]\n",
    "\n",
    "    def get_start(self, item):\n",
    "        \"\"\"Returns the start position of the CKY item\"\"\"\n",
    "        return item[0]\n",
    "\n",
    "    def get_end(self, item):\n",
    "        \"\"\"Returns the end position of the CKY item\"\"\"\n",
    "        return item[2]\n",
    "\n",
    "    def is_goal(self, item, n):\n",
    "        \"\"\"\n",
    "        Test if this is a goal item\n",
    "        :param item: a CKY item as returned by `make_item`\n",
    "        :param n: length of the input sentence\n",
    "        :returns: True if GOAL, False otherwise\n",
    "        \"\"\"\n",
    "        # TYPE YOUR SOLUTION\n",
    "        pass\n",
    "\n",
    "    def axioms(self, sentence):\n",
    "        \"\"\"\n",
    "        Return the axioms compatibe with a certain input sentence\n",
    "        :param sentence: a sequence of Terminal objects\n",
    "        :returns: a set of items \n",
    "        \"\"\"\n",
    "        # TYPE YOUR SOLUTION\n",
    "        pass\n",
    "\n",
    "    def merge(self, item1, item2):\n",
    "        \"\"\"\n",
    "        Returns a set of inferred items based on the MERGE rule.\n",
    "        :param item1: a CKY item as created by `make_item`\n",
    "        :param item2: a CKY item as created by `make_item`\n",
    "\n",
    "        Note that you need to make sure whether the MERGE rule applies at all, \n",
    "         that is, item1 and item2 need to be adjacent. Also note that the items\n",
    "         may have been passed to this function in opposite order. \n",
    "        Examples of valid applications of the merge rule:\n",
    "             * [1, X, 2] [2, Y, 3]: this is valid because one item ends where the other starts\n",
    "             * [2, Y, 3] [1, X, 2]: this is also valid! because again one item (the second) ends where the other (the first) starts\n",
    "\n",
    "        :returns: a set of items inferred by application of MERGE\n",
    "        \"\"\"\n",
    "        # TYPE YOUR SOLUTION\n",
    "        pass\n",
    "\n",
    "    def recognise(self, sentence):\n",
    "        \"\"\"\n",
    "        Test whether a sentence belongs to the language of the grammar.\n",
    "\n",
    "        Recall the general strategy:\n",
    "            - maintain an agenda of items\n",
    "            - explore items exhaustively\n",
    "            - never put in the agenda an item that you have already discovered\n",
    "            - stop whenever you manage to prove the GOAL item            \n",
    "\n",
    "        :param sentence: a sequence of Terminal objects\n",
    "        :returns: True if S =>* sentence, False otherwise.\n",
    "        \"\"\"\n",
    "        # TYPE YOUR SOLUTION\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A toy CNF grammar which you can use to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_cnf_grammar():\n",
    "    # Some symbols\n",
    "    S = Nonterminal('S')\n",
    "    NP = Nonterminal('NP')\n",
    "    N = Nonterminal('N')\n",
    "    VP = Nonterminal('VP')\n",
    "    V = Nonterminal('V')\n",
    "    CC = Nonterminal('CC')\n",
    "    cnf = CFG(S)\n",
    "    # Phrasal rules\n",
    "    cnf.add(Rule(S, [NP, VP]))\n",
    "    cnf.add(Rule(S, [NP, V]))\n",
    "    cnf.add(Rule(S, [N, VP]))\n",
    "    cnf.add(Rule(S, [N, V]))\n",
    "    cnf.add(Rule(NP, [N, NP]))\n",
    "    cnf.add(Rule(NP, [CC, N]))\n",
    "    cnf.add(Rule(VP, [V, VP]))\n",
    "    cnf.add(Rule(VP, [CC, V]))\n",
    "    # Preterminal rules\n",
    "    cnf.add(Rule(N, [Terminal('cats')]))\n",
    "    cnf.add(Rule(N, [Terminal('dogs')]))\n",
    "    cnf.add(Rule(N, [Terminal('birds')]))\n",
    "    cnf.add(Rule(V, [Terminal('run')]))\n",
    "    cnf.add(Rule(V, [Terminal('bark')]))\n",
    "    cnf.add(Rule(V, [Terminal('meow')]))\n",
    "    cnf.add(Rule(V, [Terminal('chirp')]))\n",
    "    cnf.add(Rule(CC, [Terminal('and')]))\n",
    "    return cnf\n",
    "\n",
    "\n",
    "print(get_toy_cnf_grammar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cky = CKYRecogniser(get_toy_cnf_grammar())\n",
    "# Try these\n",
    "# cky.recognise([Terminal(w) for w in 'cats and dogs run'.split()])\n",
    "# cky.recognise([Terminal(w) for w in 'cats and dogs jump'.split()])\n",
    "# And some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a name=\"pcfg\"> PCFG\n",
    "\n",
    "A probabilistic CFG is a simple extension to CFGs where we assign a joint probability distribution over the space of context-free *derivations*. \n",
    "\n",
    "A random **derivation** $D = \\langle R_1, \\ldots, R_m \\rangle$ is a sequence of $m$ *random rule applications*.\n",
    "A random rule is a pair of a random LHS nonterminal $V$ and a random RHS sequence $\\beta$, where $V \\rightarrow \\beta$ corresponds to a valid rule in the grammar.\n",
    "\n",
    "We assume that a derivation is generated one rule at a time and each rule is generated independently. Moreover, the probability value of a rule is given by a conditional probability distribution over RHS sequences given LHS nonterminal. \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P_{D|M}(r_1^m|m) &= \\prod_{i=1}^m P_R(r_i) \\\\\n",
    " &= \\prod_{i=1}^m P_{\\text{RHS}|\\text{LHS}}(\\beta_i | v_i)\\\\\n",
    " &= \\prod_{i=1}^m \\text{Cat}(\\beta_i | \\boldsymbol \\theta^{v_i})\\\\\n",
    " &= \\prod_{i=1}^m \\theta_{v_i \\rightarrow \\beta_i}\\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-7\" style=\"color:red\">**Exercise 6-7**</a> **[1 point]** How many parameters are there in this model? Express your answer in big-o-notation as a function of sizes of terminal and nonterminal sets as well as the grammar's arity (note that we are not assuming CNF grammars here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-8\" style=\"color:red\">**Exercise 6-8**</a> **[2 points]** If our grammar were in CNF form, then how many parameters would we have?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "We can implement PCFGs rather easily by pairing a CFG grammar with a dictionary mapping from rules to their probabilities. But we must remember that for each given LHS symbol, the probability values of all of its rewriting rules must sum to 1.\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{\\beta} \\theta_{v \\rightarrow \\beta} = 1\n",
    "\\end{equation}\n",
    "\n",
    "Below we have an example of how we could code the necessary cpds for our toy grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_pcfg():\n",
    "    cfg = get_toy_grammar(np_cc_np=True, vp_cc_vp=True)\n",
    "    # Some symbols\n",
    "    S = Nonterminal('S')\n",
    "    NP = Nonterminal('NP')\n",
    "    N = Nonterminal('N')\n",
    "    VP = Nonterminal('VP')\n",
    "    V = Nonterminal('V')\n",
    "    CC = Nonterminal('CC')\n",
    "    p_R = {\n",
    "        S: {\n",
    "            Rule(S, [NP, VP]): 1.0,\n",
    "        },\n",
    "        # NP -> *\n",
    "        NP: {\n",
    "            Rule(NP, [NP, CC, NP]): 0.2,\n",
    "            Rule(NP, [N]): 0.8,\n",
    "        },\n",
    "        # VP -> *\n",
    "        VP: {\n",
    "            Rule(VP, [VP, CC, VP]): 0.1,\n",
    "            Rule(VP, [V]): 0.9,\n",
    "        },\n",
    "        # N -> *\n",
    "        N: {\n",
    "            Rule(N, [Terminal('cats')]): 0.3,\n",
    "            Rule(N, [Terminal('dogs')]): 0.5,\n",
    "            Rule(N, [Terminal('birds')]): 0.2,\n",
    "        },\n",
    "        # V -> *\n",
    "        V: {\n",
    "            Rule(V, [Terminal('run')]): 0.4,\n",
    "            Rule(V, [Terminal('bark')]): 0.25,\n",
    "            Rule(V, [Terminal('meow')]): 0.25,\n",
    "            Rule(V, [Terminal('chirp')]): 0.1,\n",
    "        },\n",
    "        CC: {\n",
    "            Rule(CC, [Terminal('and')]): 1.0,\n",
    "        }\n",
    "    }\n",
    "    return cfg, p_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg, p_R = get_toy_pcfg()\n",
    "print('CFG')\n",
    "print(cfg)\n",
    "print('P_R')\n",
    "for lhs, cpd in p_R.items():\n",
    "    total_prob = 0.0\n",
    "    for rule, prob in cpd.items():\n",
    "        print('%.2f' % prob, rule)\n",
    "        total_prob += prob\n",
    "    print('LHS %s Total probability %.2f' % (lhs, total_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-9\" style=\"color:red\">**Exercise 6-9**</a> **[3 points]** Implement a function that generates samples from the grammar respecting a certain specification of $P_R$. *Tip:* this is very similar to uniform generation, but instead of choosing rules uniformly at random, we should choose them according to the given cpds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(cfg: CFG, cpds):\n",
    "    \"\"\"\n",
    "    Given a CFG,returns a derivation (and its yield) sampled top-down uniformly at random.\n",
    "\n",
    "    Recall that a derivation is a depth-first traversal, that is, \n",
    "        we always rewrite the leftmost nonterminal symbol first.\n",
    "\n",
    "    :param cfg: a CFG grammar    \n",
    "    :param cpds: a dictionary mapping a LHS to a cpd\n",
    "        where each cpd is a dictionary mapping rules to probabilities\n",
    "        see the output of get_toy_pcfg above\n",
    "    :returns: derivation (sequence of rules), yield (sequence of words)\n",
    "    \"\"\"\n",
    "    # TYPE YOUR SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-10\" style=\"color:red\">**Exercise 6-10**</a> **[2 points]** A correct implementation should produce strings with certain patterns.  Get a large sample from your grammar (e.g. 1000 samples) and verify the following patterns.\n",
    "\n",
    "* **[1 point]** Sentences of the kind: \"... and ...\". The terminal `and` can only be generated by `CC`, which appears in 20% of the derivations containing `NP` and 10% of the derivations containing `VP`. Since every derivation contains at least one NP and at least one VP, we expect about 30% of the strings to match this pattern.\n",
    "* **[1 point]** According to our toy grammar, every sentence has at least one verb, and we can see from the distribution over verbs that running is the most popular thing to do amongst these animals. We expect about 40% of the strings to contain `run`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of this this lab we will then consider the question:\n",
    "* how do estimate $P_R$ from data?\n",
    "\n",
    "Next lab we will investigate questions such as:\n",
    "* how do parse existing sentences with a CFG?\n",
    "* how do we predict a good tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a name=\"mle\"> MLE\n",
    "\n",
    "Now we can get a treebank from NLTK and compute maximum likelihood estimates for the various Categorical distributions.\n",
    "\n",
    "For a given rule $v \\rightarrow \\beta$, the maximum likelihood estimate is\n",
    "\\begin{equation}\n",
    "\\theta_{v \\rightarrow \\beta} = \\frac{\\text{count}_R(v \\rightarrow \\beta)}{\\sum_{\\beta'} \\text{count}_R(v \\rightarrow \\beta')} = \\frac{\\text{count}_R(v \\rightarrow \\beta)}{\\text{count}_V(v) }\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We will work with **smoothed** lexical distributions. This means we will only smooth **preterminal** rules, that is,  rules where the LHS is a POS category and the RHS is a single terminal. \n",
    "\n",
    "For such preterminal rules the smoothed MLE solution is\n",
    "\\begin{equation}\n",
    "\\theta_{v \\rightarrow x} = \\frac{\\text{count}_R(v \\rightarrow \\beta) + \\alpha}{\\text{count}_V(v) +|\\Sigma|\\times \\alpha }\n",
    "\\end{equation}\n",
    "\n",
    "For smoothing to work we have to add an unknown word to the vocabulary of terminals of the grammar, as well as preterminals rules that map from POS categories to this unknown word. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will use annotated data from NLTK. For that we have a few example of how to read data and we have provided code that converts from nltk internal objects to objects we have defined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "treebank = treebank.parsed_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parses = treebank[:3000]\n",
    "print(len(training_parses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parses = treebank[3000:3900]\n",
    "len(test_parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_parses = treebank[3900:]\n",
    "len(dev_parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can visualise a tree as an image\n",
    "dev_parses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and you can also visualise it as a nested bracketed string\n",
    "print(dev_parses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we convert from nltk CFG production object to our own Rule object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.grammar import Nonterminal as NLTKNonterminal\n",
    "\n",
    "\n",
    "def convert_nltk_rule(nltk_rule) -> Rule:\n",
    "    \"\"\"Here we convert from nltk production object to our own Rule object\"\"\"\n",
    "    lhs = Nonterminal(str(nltk_rule.lhs()))\n",
    "    rhs = []\n",
    "    for sym in nltk_rule.rhs():\n",
    "        if isinstance(sym, NLTKNonterminal):\n",
    "            rhs.append(Nonterminal(str(sym)))\n",
    "        else:\n",
    "            rhs.append(Terminal(str(sym)))\n",
    "    return Rule(lhs, rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a CFG by inspecting the rules used in a treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cfg(parse_trees, max_arity=4, start=Nonterminal('S')):\n",
    "    \"\"\"\n",
    "    Here we make a CFG by collecting rules used to in a treebank.\n",
    "    We impose a maximum arity because some annotated rules are incredibly long!\n",
    "\n",
    "    :param parse_trees: a collection of nltk trees\n",
    "    :param max_arity: maximum arity we want for our grammar (we discard rules longer than this)\n",
    "    :param start: the nonterminal that starts derivations in this grammar\n",
    "        - Penn treebank uses S as start symbol\n",
    "    :returns: a CFG object containing all rules used in parse_trees if they are not longer than what we want\n",
    "    \"\"\"\n",
    "    cfg = CFG(start)\n",
    "    for nltk_tree in parse_trees:\n",
    "        for nltk_rule in nltk_tree.productions():\n",
    "            rule = convert_nltk_rule(nltk_rule)\n",
    "            if rule.arity <= max_arity:\n",
    "                cfg.add(rule)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_grammar = make_cfg(training_parses, max_arity=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PTB')\n",
    "print(len(ptb_grammar), 'rules')\n",
    "print(len(ptb_grammar.terminals), 'terminal')\n",
    "print(len(ptb_grammar.nonterminals), 'nonterminal')\n",
    "print(len(ptb_grammar.preterminals), 'preterminals')\n",
    "print('arity', ptb_grammar.arity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation** Now you will implement MLE for PCFGs with lexical smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex6-11\" style=\"color:red\">**Exercise 6-11**</a> **[4 points]** MLE for PCFGs\n",
    "\n",
    "* **[1 point]** Start by implementing `count_rules` below to gather counts from the data. Organise counts in a data structure similar to our $P_R$ in `get_toy_pcfg`, that is, a dictionary of dictionaries where the outer dict maps LHS to Rule objects, the inner dict maps from a Rule object to its count in the data.\n",
    "* **[1 point]** Implement Laplace smoothing for pre-terminal rules in `add_pseudo_counts_and_unk_rules`\n",
    "* **[1 point]** Estimate MLE parameters using the treebank `training_parses` above (first 3000 examples of NLTK's PTB data), and save your MLE solutions to a file where each line contains ```PROB LHS -> RHS```, make sure your file is sorted by LHS (lexicographically) and then by probability value (best first). Example:\n",
    "\n",
    "```\n",
    "0.6 NP -> N\n",
    "0.4 NP -> D N\n",
    "0.9 S -> NP VP\n",
    "0.1 S -> VP\n",
    "0.5 VP -> V\n",
    "0.5 VP -> VP VP\n",
    "```\n",
    "* **[1 point]** Show examples of string generated by sampling from the grammar using the function `generate_sample` you implemented earlier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rules(parse_trees):\n",
    "    \"\"\"\n",
    "    Gather counts necessary for MLE.\n",
    "    :param parse_trees: a collection of nltk parse trees\n",
    "        - TIP: use the convertion function we provided\n",
    "    :returns: counts as a dictionary of dictionaries mapping from LHS to RULE to count\n",
    "    \"\"\"\n",
    "    # TYPE YOUR SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint_counts = count_rules(training_parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pseudo_counts_and_unk_rules(cfg: CFG, joint_counts, alpha: float, unk_str='<unk>'):\n",
    "    \"\"\"\n",
    "    Add pseudo counts for Laplace smoothing of pre-terminal rules.\n",
    "\n",
    "    This function should update `joint_counts` with pseudo counts whenever appropriate.\n",
    "    Recall that you have to create an unk Terminal and you have to add unk rules to the grammar.    \n",
    "\n",
    "    :param cfg: CFG\n",
    "    :param joint_counts: dictionary of dictionaries from `count_rules` above where\n",
    "        joint_counts[X][r] is the count of a rule r whose LHS is X\n",
    "    :param alpha: Laplace smoothing constant\n",
    "    :param unk_str: the string for unknown terminals\n",
    "    \"\"\"\n",
    "    # TYPE YOUR SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following\n",
    "# add_pseudo_counts_and_unk_rules(ptb_grammar, joint_counts, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_maximum_likelihood_estimates(cfg: CFG, joint_counts):\n",
    "    \"\"\"\n",
    "    Use the gathered counts to compute the maximum likelihood estimates.\n",
    "\n",
    "    :param cfg: a CFG object\n",
    "    :param joint_counts: counts as computed by `count_rules` and smoothed by `add_pseudo_counts_and_unk_rules`\n",
    "    :returns: cpds as a dictionary of dictionaries where\n",
    "        cpds[X][r] is the MLE parameter for rule r whose LHS is X\n",
    "    \"\"\"\n",
    "    # TYPE YOUR SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following\n",
    "# p_R = compute_maximum_likelihood_estimates(ptb_grammar, joint_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
