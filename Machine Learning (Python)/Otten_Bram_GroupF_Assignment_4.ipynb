{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Bram Otten\n",
    "\n",
    "### Student ID: 10992456\n",
    "\n",
    "### Group: F\n",
    "\n",
    "Please fill in you name, student ID and group above, and also edit the filename according to the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from random import uniform\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_matrix = iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means \n",
    "\n",
    "\n",
    "For this assignment the setup will be a little different from the previous weeks. Instead of incrementally writing functions to build the algorithm, it will be up to you to design the code of the algorithm from scratch. The first algorithm we will be covering is *K-means*. The pseudocode from Alpayding for this algorithm (figure 7.3) is:\n",
    "\n",
    "* Initialize $m_i$, $i$ = $1$, ... , $k$, for example, to $k$ random $x^t$\n",
    "* Repeat\n",
    "    * For all $x^t \\in X$\n",
    "        * $b_i^t \\leftarrow \\left\\{\\begin{array}{ll} 1 & if\\ \\Vert x^t − m_i \\Vert \\ =\\ min_j\\ \\Vert x^t − m_j \\Vert\\\\ \n",
    "            0 & otherwise \\\\ \\end{array}\\right.$\n",
    "    * For all $m_i$, $i$ = $1$, ... , $k$\n",
    "        * $m_i \\leftarrow \\sum_t b_i^t x^t / \\sum_t b_i^t$\n",
    "* Until $m_i$ converge\n",
    "\n",
    "This video by [Andrew on the topic](https://www.youtube.com/watch?v=6u19018FeHg&index=78&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW) might also be helpful to understand what the algorithm does. Again the notation is a little different, but the ideas are exactly the same.\n",
    "\n",
    "## Implementing the algorithm [10 pts]\n",
    "\n",
    "For your code, make **functions** for each of the following components of the k-means algorithm:\n",
    "\n",
    "* Randomly initialize means from the data [1pt]\n",
    "* Compute the distance between 2 points [1pt]\n",
    "* Compute the matrix $b$ containing the assignments of points to clusters, based on the current means [2pts]\n",
    "* Compute the matrix $m$ containing the computed mean vectors, based on the current assignment of clusters [2pts]\n",
    "* Plot the means (as x's) and their assigned points (as dots), with a different color for each cluster (here you may assume the points will all be 2-dimensional, to allow them to be plotted) [2pts]\n",
    "* Determine if the algorithm has converged based on the sets of current and new means [1pt]\n",
    "* Combine all these functions in a general k-means function [1pt]\n",
    "\n",
    "## Showing the results [1 pt]\n",
    "\n",
    "To show your code works, run the algorithm on the Iris dataset using only the last 2 variables of the data. Use a value of 3 for $k$ and plot the means with their assignments. Start by running the algorithm step by step and plotting for each step. Start with the random means and then show at least 2 more steps, to show the means moving as the algorithm iterates. Finally, show the resulting plot where the means have converged and the algorithm has stopped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_means(data, k):\n",
    "    means = []\n",
    "    minimums = []\n",
    "    maximums = []\n",
    "    for column in data.transpose():\n",
    "        minimums.append(np.amin(column))\n",
    "        maximums.append(np.amax(column))\n",
    "\n",
    "    for class_index in range(k):\n",
    "        coordinates = []\n",
    "        for axis in range(len(minimums)):\n",
    "            coordinates.append(uniform(minimums[axis],\n",
    "                                       maximums[axis]))\n",
    "        means.append(coordinates)\n",
    "\n",
    "    means = np.array(means)\n",
    "    return means\n",
    "\n",
    "\n",
    "def dist_two_points(A, B):\n",
    "    total = 0\n",
    "    dimension = len(A)\n",
    "    for i in range(dimension):\n",
    "        total += (A[i] - B[i]) ** 2\n",
    "    return total ** 0.5\n",
    "\n",
    "\n",
    "def compute_b_matrix(data, means, k):\n",
    "    best_class_list = []\n",
    "    for datapoint in data:\n",
    "        best_class = -1\n",
    "        smallest_dist = float('inf')\n",
    "        for i in range(k):\n",
    "            cur_dist = dist_two_points(datapoint, means[i])\n",
    "            if cur_dist < smallest_dist:\n",
    "                smallest_dist = cur_dist\n",
    "                best_class = i\n",
    "        best_class_list.append(best_class)\n",
    "    return np.array(best_class_list)\n",
    "\n",
    "\n",
    "def compute_m_matrix(data, b_matrix, k, old_m_matrix):\n",
    "    dimensions = len(data[0])\n",
    "    m_matrix = [[0 for i in range(dimensions)] for j in range(k)]\n",
    "    counter = [0 for j in range(k)]\n",
    "    for i in range(len(data)):\n",
    "        c_i = b_matrix[i]\n",
    "        data_i = data[i]\n",
    "        m_matrix[c_i] += data_i\n",
    "        counter[c_i] += 1\n",
    "\n",
    "    for i in range(k):\n",
    "        if counter[i] > 0:\n",
    "            m_matrix[i] = np.divide(m_matrix[i], counter[i])\n",
    "        else:\n",
    "            # Stupid quick solution, but the old m[i] is useless\n",
    "            m_matrix[i] = data[0]\n",
    "\n",
    "    return np.array(m_matrix)\n",
    "\n",
    "\n",
    "def determine_convergence(old, new):\n",
    "    return np.array_equal(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printy_compute_k_means(data, k):\n",
    "    m_matrix = initialize_means(data, k)\n",
    "    print(\"Means:\")\n",
    "    print(m_matrix)\n",
    "    print(\"---\")\n",
    "    print(\"Dist\", data[0], m_matrix[0], \"=\",\n",
    "          dist_two_points(data[0], m_matrix[0]))\n",
    "    print(\"---\")\n",
    "    print(\"Data and associated b:\")\n",
    "    print(data[0:5])\n",
    "    b_matrix = compute_b_matrix(data, m_matrix, k)\n",
    "    print(b_matrix[0:5])\n",
    "    print(\"---\")\n",
    "    print(\"New means (or m_matrix):\")\n",
    "    m_matrix = compute_m_matrix(data, b_matrix, k, m_matrix)\n",
    "    print(m_matrix)\n",
    "    print(\"---\")\n",
    "    print(\"Can do that again:\")\n",
    "    b_matrix = compute_b_matrix(data, m_matrix, k)\n",
    "    new_m_matrix = compute_m_matrix(data, b_matrix, k, m_matrix)\n",
    "    print(new_m_matrix)\n",
    "    print(\"---\")\n",
    "    print(\"Was I already done?\",\n",
    "          determine_convergence(m_matrix, new_m_matrix))\n",
    "\n",
    "\n",
    "def plot_k_means(data, b_matrix, m_matrix, k):\n",
    "    # We can assume 2D data here.\n",
    "    k_list_x1 = [[] for i in range(k)]\n",
    "    k_list_x2 = [[] for i in range(k)]\n",
    "    for i in range(len(b_matrix)):\n",
    "        k_list_x1[b_matrix[i]].append(data[i, 0])\n",
    "        k_list_x2[b_matrix[i]].append(data[i, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(k):\n",
    "        plt.scatter(k_list_x1[i], k_list_x2[i],\n",
    "                    label='Class %d' % i, s=8)\n",
    "    plt.scatter(m_matrix[:, 0], m_matrix[:, 1],\n",
    "                marker='x', color='black')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_k_means(data, k, plotting=True):\n",
    "    m_matrix = initialize_means(data, k)\n",
    "    b_matrix = compute_b_matrix(data, m_matrix, k)\n",
    "    new_m_matrix = compute_m_matrix(data, b_matrix, k, m_matrix)\n",
    "    while determine_convergence(m_matrix, new_m_matrix) == False:\n",
    "        if plotting:\n",
    "            plot = plot_k_means(data, b_matrix, m_matrix, k)\n",
    "            plt.show(plot)\n",
    "        m_matrix = new_m_matrix\n",
    "        b_matrix = compute_b_matrix(data, m_matrix, k)\n",
    "        new_m_matrix = compute_m_matrix(data, b_matrix, k, m_matrix)\n",
    "    return m_matrix\n",
    "\n",
    "\n",
    "iris_data = np.array(X_matrix[:, 2:4])\n",
    "iris_k = 3\n",
    "# printy_compute_k_means(iris_data, iris_k)\n",
    "# print(\"***\")\n",
    "print(\"Last two iris columns %d means clustered:\" % iris_k)\n",
    "iris_k_means = compute_k_means(iris_data, iris_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method [4 pts]\n",
    "\n",
    "For this dataset we have the benefit of already knowing the number of clusters. However, there are even some things we can do if the number clusters is not known or set before hand. One possbile approach is the *elbow method*. Watch the video from Andrew on the topic below or find another resource describing it.\n",
    "\n",
    "[Choosing the number of clusters?](https://www.youtube.com/watch?v=izCbbMbRWHw&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=81)\n",
    "\n",
    "Now extend your implementation to include the following:\n",
    "\n",
    "* Create a sensible cost function using the data and the current values of $b$ and $m$, that steps of the algorithm will minimize. [1 pt]\n",
    "* Create a function to compute the converged cost of a specific value of $k$ repeatedly and averaging this. [1 pt]\n",
    "* Running your code many times will most likely result in an error in your `compute_means` function at some point. Find out what is causing the error and create a new version of `compute_means` that solves this problem in some sensible way. [1 pts]\n",
    "* Compute the average cost for k values $1$ to $10$ and combine them in a plot. Briefly discuss if this plot corresponds with your expectations. [1 pt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_cost(data, k, m, b):\n",
    "    total = 0.0\n",
    "    for i in range(len(data)):\n",
    "        m_i = m[b[i]]\n",
    "        total += dist_two_points(data[i], m_i)\n",
    "    return total / len(data)\n",
    "\n",
    "\n",
    "def clustering_repeater(data, k, repeats=3):\n",
    "    cost = 0.0\n",
    "    for i in range(repeats):\n",
    "        m_i = compute_k_means(data, k, False)\n",
    "        b_i = compute_b_matrix(data, m_i, k)\n",
    "        cost += clustering_cost(data, k, m_i, b_i)\n",
    "    return cost / k\n",
    "\n",
    "\n",
    "def average_costs_plots(data, max_k=10):\n",
    "    cost_list = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        k_cost = clustering_repeater(data, k)\n",
    "        cost_list.append(k_cost)\n",
    "    plt.figure()\n",
    "    plt.title('Normalized Euclidean cost (y-axis) vs k (x-axis)')\n",
    "    plt.plot(range(1, max_k + 1), cost_list)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "average_costs_plots(iris_data)\n",
    "\n",
    "print(\"These are (usually) the results I would have expected.\")\n",
    "print(\"Costs decrease as k increases, because a higher k means\")\n",
    "print(\"more means and more means means it's 'harder' for\")\n",
    "print(\"datapoints to be far from their mean.\")\n",
    "print()\n",
    "print(\"But sometimes a higher k is associated with a higher cost.\")\n",
    "print(\"I'll conveniently ascribe that to randomness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN [5 pts]\n",
    "\n",
    "The final algorithm of this week is kNN, the quintessential non-parametric classification algorithm. To implement this you may use any resource on the algorithm you prefer, be it the description in the sides, in *Alpaydin*, a video on the topic or some other content you found. The division of functions entirely up to you as well, you will only be scored on the following components being present:\n",
    "\n",
    "* A functioning implementation of kNN [2 pts]\n",
    "* A version of kNN that weights the contribution of k datapoints based on distance to the point being evaluated [1 pt]\n",
    "* A sufficient documentation of your functions and the choices you made in your design [1 pt]\n",
    "* A structured comparision of the 2 algorithms using different values of K on the Iris dataset [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_split(data, ratio=0.5):\n",
    "    split_data = np.copy(data)\n",
    "    np.random.shuffle(split_data)\n",
    "    split_index = int(len(data) * ratio)\n",
    "    return (split_data[:split_index, :], split_data[split_index:, :])\n",
    "\n",
    "\n",
    "def get_k_nearest_list(ref, data, k):\n",
    "    n = len(data)\n",
    "    nn_list = {}\n",
    "    for j in range(n):\n",
    "        nn_list[j] = dist_two_points(ref, data[j])\n",
    "    kn_list.append(sorted(nn_list, key=nn_list.get)[1:k + 1])\n",
    "    return kn_list\n",
    "\n",
    "\n",
    "def knn_label(train_data, train_labels,\n",
    "              val_data, k, weighted=False):\n",
    "    val_data = val_data.tolist()\n",
    "    n = len(train_data)\n",
    "    n_classes = np.amax(train_labels)\n",
    "    label_list = []\n",
    "    for ref in val_data:\n",
    "\n",
    "        # First, get k closest points.\n",
    "        distances = {}\n",
    "        for i in range(n):\n",
    "            distances[i] = dist_two_points(ref, train_data[i])\n",
    "        sorted_dist = sorted(distances, key=distances.get)\n",
    "        closest = sorted_dist[:k]\n",
    "\n",
    "        # Figure out which class is more likely based on k neighbours.\n",
    "        # My strategy is something like: get labels, return most\n",
    "        # occuring one. If weighted,\n",
    "        stupid_list = []\n",
    "        for j in range(k):\n",
    "            j_n_label = train_labels[closest[j]]\n",
    "            stupid_list.append(j_n_label)\n",
    "        occ = np.ndarray.tolist(np.bincount(stupid_list,\n",
    "                                            minlength=n_classes))\n",
    "        if weighted:\n",
    "            for i in range(len(closest)):\n",
    "                occ_i = train_labels[closest[j]]\n",
    "                d = distances[closest[i]]\n",
    "\n",
    "                # Now, the glorious weighting:\n",
    "                occ[occ_i] -= d * 2\n",
    "                # I didn't see much in the book or slides.\n",
    "\n",
    "        best_label = np.argmax(occ)\n",
    "        label_list.append(best_label)\n",
    "    return label_list\n",
    "\n",
    "\n",
    "(iris_data_train, iris_data_val) = validation_split(iris_data)\n",
    "for k in [2, 3, 5, 10]:\n",
    "    k_means = compute_k_means(iris_data_train, k, False)\n",
    "    k_means_labels = compute_b_matrix(iris_data_val, k_means, k)\n",
    "    k_means_cost = clustering_cost(iris_data_val, k,\n",
    "                                   k_means, k_means_labels)\n",
    "\n",
    "    # Here, I'll complain about the lack of mentioning\n",
    "    # how to deal with the init (/the switch to supervision).\n",
    "    # I'm using the k means' training stuff now.\n",
    "\n",
    "    k_nn_labels = knn_label(iris_data_train, k_means_labels,\n",
    "                            iris_data_val, k)\n",
    "    k_nn_labels_w = knn_label(iris_data_train, k_means_labels,\n",
    "                              iris_data_val, k, True)\n",
    "    k_nn_cost = clustering_cost(iris_data_val, k,\n",
    "                                k_means, k_nn_labels)\n",
    "    k_nn_cost_w = clustering_cost(iris_data_val, k,\n",
    "                                  k_means, k_nn_labels_w)\n",
    "\n",
    "    print(\"The %d-means algorithm returned\" % k)\n",
    "    print(k_means)\n",
    "    print(\"For a cost of\", k_means_cost)\n",
    "    print()\n",
    "    print(\"The %d-NN algorithm achieved a cost of\" % k, k_nn_cost)\n",
    "    print(\"Or, weighted:\", k_nn_cost_w)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}