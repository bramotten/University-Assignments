{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bram Otten\n",
    "\n",
    "10992456\n",
    "\n",
    "Group F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "For this assignment we will implement a basic neural network. The notation used for this in *Alpaydin* is good to help you understand the correlation between a single perceptron and linear discrimination functions, and to understand the derivates that are the basis for backpropagation in a neural network. However, it does not contain enough information to actually build a complete multilayer perceptron.\n",
    "\n",
    "For this we will use a different notation, also used by *Andrew Ng* in his videos. The benefit of this notation is that is complete, and thus translating it to data operations for the implementation is relatively straightforward. Also the notation lends itself well to using matrix multiplications, meaning a lot of the operations can actually be simplified to a couple of efficient steps.\n",
    "\n",
    "This does however mean it might take some figuring out how to map the equations from *Alpaydin* to the equations here. Mostly this will just be separating the steps in a different way and using different indices, but as stated, this will make implementing a lot easier. Plus, neural networks can be somewhat unintuitive in the beginning, so having another perspective might help make things click in your own neural network.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "Lets start by zooming in on the parameters for just one node. When using the *Sigmoid* activation function (one of the most common types) for a single node perceptron, the combined equations will correspond exactly with the *Logistic Regression* model from Chapter 10. Watch these 2 videos for the idea behind *Logistic Regression* for classification or why the *Sigmoid* function is used for this.\n",
    "\n",
    "* [Learning a classification function](https://www.youtube.com/watch?v=K5oZM1Izn3c&index=33&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
    "* [Logistic regression function](https://www.youtube.com/watch?v=WiDuvuM1JyI&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=34)\n",
    "\n",
    "### Sigmoid Function [1 pt]\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$g(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "Write the function `sigmoid_func` that can take single value `X`, or even a vector of values `X` and compute the *Sigmoid* function for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression  [1 pt]\n",
    "\n",
    "The model for *Logistic Regression* as given by *Alpaydin* is\n",
    "\n",
    "(10.37) $$y^t = \\frac{1}{1 + e^{-(w^T x^t + w_0 )}}$$\n",
    "\n",
    "This model is can compute every $y^t$ value for a some matrix of $X$ values, however, as you might have noticed in the *Logistic Regression* video, there is an alternative. If we expand the matrix $X$ with a column of $1$ values used for the bias inputs, just like with the *Polynomial Regression* $D$ matrix, then the equation simplifies to\n",
    "\n",
    "$$y^t = \\frac{1}{1 + e^{-w^T x^t}}$$\n",
    "\n",
    "Now given some vector of weights $w$ and a matrix of inputs $X$, computing the inputs for the *Sigmoid* function for each of the $X$ rows just be becomes a single matrix multiplication. Given that your `sigmoid_func` can also handle vectors, transforming this vector into a vector of *Logistic Regression* outputs will also just be one function call.\n",
    "\n",
    "Write the function `add_bias`, which takes a matrix of `X` values, and returns that same matrix with a column of ones appended to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding to a layer of nodes\n",
    "\n",
    "Until now we have only considered single node outputs, but *Neural Networks* are built from layers of these nodes. Each of these layers consists of a combination of individual *Sigmoid* nodes. Using multiple nodes on the output layer, it is possible to classify multiple classes, instead of just $0$ and $1$. See the video below for a brief example.\n",
    "\n",
    "* [Multiclass classification](https://www.youtube.com/watch?v=HzpptanxP6A&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=50)\n",
    "\n",
    "As each of the these *Sigmoid* nodes needs a set of weights, the weights for a single layer now become a matrix $\\Theta$, where each row contains the weights for one of the sigmoids. In a complete network this matrix is indexed as follows,\n",
    "\n",
    "$$\\Theta^j_{ki}$$\n",
    "\n",
    "is the weight from $i^{th}$ node in layer $j$ to the $k^{th}$ node in layer $j+1$. As we will start by only considering a single layer version, we will only need $\\Theta^0_{ki}$.\n",
    "\n",
    "### Initializing weights [1 pt]\n",
    "\n",
    "To start producing outputs with a one layer network, we need values for the $\\Theta^0$ matrix. When creating a *Neural Network*, it important to use random starting values for the weights. For more context on why this useful, you can watch:\n",
    "\n",
    "* [Random initialization](https://www.youtube.com/watch?v=NhgB6FLyHJc&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=56)\n",
    "\n",
    "Write the function `one_layer_init`, which takes the number of input and output nodes and returns a matrix of weights of the correct dimensions. Each weight should be randomly initialized using a uniform distribution over the range $[-0.3, +0.3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing activations [1 pt]\n",
    "\n",
    "Now lets start combining these pieces and work on the actual *Neural Network* computation model. This model will not be able to \"learn\" yet, just compute outputs given inputs and set of weights. Hopefully you already have a bit of an idea of what this would look like for a 1 layer network, but here we will include the complete multilayer version, in order to get a sense of the bigger picture too. Watch these 2 videos on the model representation:\n",
    "\n",
    "* [Model representation I](https://www.youtube.com/watch?v=wnSol2JRZeY&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=46)\n",
    "* [Model representation II](https://www.youtube.com/watch?v=vuhueI_7324&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=47)\n",
    "\n",
    "So, for the multilayer case, the outputs from one layer just become the inputs for the next layer. Completing the system of equations; the activation at the input layer just consists of the bias term and the values for each of the input variables.\n",
    "\n",
    "$$A^0 = [1, x_1, x_2 \\dots x_N]$$\n",
    "\n",
    "The input for each next layer is just the sum of the activations from the previous layer multiplied by their weights\n",
    "\n",
    "$$Z^{j+1}_k = \\sum_i A^j_i\\Theta^j_{ki}$$\n",
    "\n",
    "And then the activation at each layer is computed by taken the *Sigmoid* function of the node input\n",
    "\n",
    "$$A^j_i = g(Z^j_i)$$\n",
    "\n",
    "Write the function `compute_layer`, which takes a matrix for $A^j$ values (with each row being a different training example) and matrix $\\Theta^j$ and return next the matrix $A^{j+1}$. Next write the function `one_layer_output`, which takes a matrix of training examples $X$ and a matrix of weights $\\Theta^0$ and returns a matrix of computed outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "def add_bias(X):\n",
    "    return np.append(X, np.ones((len((X)), 1)), axis=1)\n",
    "\n",
    "\n",
    "def one_layer_init(input_size, output_size, wr=0.3):\n",
    "    # Dimensions: input x output\n",
    "    W = []\n",
    "    for _ in range(input_size):\n",
    "        W.append([np.random.uniform(-wr, wr) for _ in range(output_size)])\n",
    "    return np.matrix(W)\n",
    "\n",
    "\n",
    "def compute_layer(A_j, Theta_j):\n",
    "    return np.dot(A_j, Theta_j)\n",
    "\n",
    "\n",
    "def one_layer_output(X, Theta_0):\n",
    "    # Maybe add_bias(X).\n",
    "    return sigmoid_func(compute_layer(X, Theta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer computations [1 pt]\n",
    "\n",
    "In order to do work with multilayer networks, we will need the complete matrix $\\Theta$. Write the function `n_layer_init`, which takes a positional argument for the size of every layer and returns an array of randomly initialized weight matrices of the correct dimensions to connect each of the layers. E.g. `n_layer_init(3, 4)` should create network with 3 inputs and 4 outputs, `n_layer_init(2, 5, 1)` should create a network with 2 inputs, 5 hidden nodes, and 1 output, etc.\n",
    "\n",
    "Now write the function `n_layer_output`, which takes a matrix $X$ and an array of matrices $\\Theta$ and returns the output matrix for that network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with boolean functions [1 pt]\n",
    "\n",
    "With all this these functions written, it might be a good time to reflect on what it is that such a network would actually compute and what these weights in the network could represent, if anything. A helpful step might be this video, where the weights for simple boolean functions are determined by hand.\n",
    "\n",
    "* [Intuitions 1](https://www.youtube.com/watch?v=BhWlHvjEn3s&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=48)\n",
    "\n",
    "These boolean functions are some of the easiest problems you can compute on a neural network, and so they are also great for testing. Create a matrix $X$ containing the 4 possible boolean input pairs for a function, randomly initialize several networks and compute the corresponding output. Create a network with no hidden layers, a network with 1 hidden layer and a network with 3 hidden layers, compute the output for each and print the results.\n",
    "\n",
    "Next take the weights given for the *AND* function in the video and set them in a matrix $\\Theta^0$. Compute the output for this matrix and show the results. Do they look like you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_init(*layer_sizes):\n",
    "    weights = []\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        weights.append(one_layer_init(layer_sizes[i - 1],\n",
    "                                      layer_sizes[i]))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def n_layer_output(X, Theta):\n",
    "    for layer_index in range(len(Theta)):\n",
    "        X = one_layer_output(X, Theta[layer_index])\n",
    "    return X\n",
    "\n",
    "\n",
    "XOR = np.matrix([[1, 1],\n",
    "                 [1, 0],\n",
    "                 [0, 1],\n",
    "                 [0, 0]])\n",
    "bXOR = add_bias(XOR)\n",
    "(_, nbRows) = bXOR.shape\n",
    "ngWeights = [np.matrix([[20],\n",
    "                        [20],\n",
    "                        [-30]])]\n",
    "\n",
    "print(\"Ng's weights:\")\n",
    "ngOut = n_layer_output(bXOR, ngWeights)\n",
    "refXOR = np.matrix([[0], [1], [1], [0]])\n",
    "print(ngOut)\n",
    "print()\n",
    "print(\"Random weights, varying amounts of hidden layers:\")\n",
    "print(n_layer_output(bXOR, n_layer_init(nbRows, 1)))\n",
    "print(n_layer_output(bXOR, n_layer_init(nbRows, nbRows, 1)))\n",
    "print(n_layer_output(bXOR, n_layer_init(nbRows, nbRows,\n",
    "                                        nbRows, nbRows, 1)))\n",
    "\n",
    "print(\"\"\"\n",
    "These results are expected. The init provides really \n",
    "small weights, so they do look a little too close to \n",
    "sigmoid(0) compared to Ng's. \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with one layer\n",
    "\n",
    "Now we have a model capable of computing outputs, given a set of inputs and weights. The interesting thing is of course if we can modify the weights to model an existing relationship between the input and output in our training data, e.g. if we could find the parameters for the *AND* function automatically.\n",
    "\n",
    "Recall from the start of the assignment that each single node in the network corresponds to a *Logistic Regression* model. We could therefore try to take the same approach to optimize the weights of the network as with *Logistic Regression*, and apply *Gradient Descent*. \n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "In *Gradient Descent* we incrementally change the weights to improve the fit on our training examples according to the cost function. The easiest case to understand what *Gradient Descent* actually does is in the case of *Linear Regression*. This is the first actual machine learning algorithm you built, in the second week of the course. What we did back then was solved at what point the partial dervatives of the cost function for the parameters $\\theta_0$ and $\\theta_1$ were 0. This type of analytical solution is possible for simple models, but not for more complex ones. In the case of *Linear Regression* we could also have used *Gradient Descent* to incrementally move $\\theta_0$ and $\\theta_1$ in the direction of the gradient and eventually settle on the same minimal cost solution. These videos can really help to build intuition on what gradient descent actually does:\n",
    "\n",
    "* [Gradient Descent 1](https://www.youtube.com/watch?v=P3K38HusyV4&index=8&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
    "* [Gradient Descent 2](https://www.youtube.com/watch?v=4SVqZaY55qo&index=9&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
    "\n",
    "### The cost function [2 pt]\n",
    "\n",
    "*Gradient Descent* work exactly the same for more complex models like *Logistic Regression* or indeed *Neural Networks*, as long as there is some function to compute the output and a cost function to optimize. The function for the classification error is defined as:\n",
    "\n",
    "$$J(\\Theta) = - \\sum_{i=1}^M \\sum_{k=1}^K Y^i_k log(H_\\Theta(X^i)_k) + (1 - Y^i_k) log(1 - H_\\Theta(X^i)_k)$$\n",
    "\n",
    "which is the sum of the classification error for all $M$ training examples and all $K$ different outputs, when comparing the current output of the network $H_\\Theta(X^i)_k$ to the target output $Y^i_k$. The video on the topic, if you want a more detailed breakdown, can be found here (note you may ignore the regularization term covered there):\n",
    "\n",
    "* [Cost Function](https://www.youtube.com/watch?v=18X68kLAfKY&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=51)\n",
    "\n",
    "Write the function `cost_function`, which takes a matrix $A$ with the network activations at the output layer (rows are outputs for different training examples, column are the different output nodes) and a matrix $Y$ of the same dimensions containing the training targets, and computes the total classification error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Terms [1 pt] \n",
    "\n",
    "So now we need partial derivatives for each of the $\\Theta$ parameters of our network. These values will be computed in 2 separate steps. First we define a term $\\delta$ that contains the partial derivatives of the cost function with respect to the nodes input\n",
    "\n",
    "$$\\delta^j_i = \\frac{\\partial J}{\\partial Z^j_i}$$\n",
    "\n",
    "which is defined for each $i^{th}$ node in each $j^{th}$ layer. Computing this term separately will make the multilayer version much easier to write. For now we will just focus on the one layer case, and in the case of an output layer, the delta term is defined as:\n",
    "\n",
    "$$\\delta^j_i = A^j_i - Y_i$$\n",
    "\n",
    "write the function `output_delta` which takes a matrix $A$ with the network activations at the output layer (rows are outputs for different training examples, column are the different output nodes) and a matrix $Y$ of the same dimensions containing the training targets, and computes the $\\delta$ terms at the output layers. This should return a matrix of the same dimensions, with a separate $\\delta$ term for each training example, for each output node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the weight updates [2 pt]\n",
    "\n",
    "With these $\\delta$ terms computed, we can now compute the derivates with respect to the $\\Theta$ parameters of the network\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\Theta_{ki}^j} = \\sum_{d=1}^M \\delta^{j+1}_kA^j_i$$\n",
    "\n",
    "summing these values over all $M$ training examples will result in the overall gradient on the cost function for $\\Theta_{ki}^j$, meaning this will be *Batch Gradient Descent*. The actual update to the weights is then just the standard *Gradient Descent* rule:\n",
    "\n",
    "$$\\Theta^j_{ki} = \\Theta^j_{ki} - \\alpha\\frac{\\partial J}{\\partial \\Theta_{ki}^j}$$\n",
    "\n",
    "Write the function `weight_update`, which should compute these values for the weights of 1 layer, i.e. the matrix $\\Theta^j$. It return the new value for $\\Theta^j$, given the old value `Theta_j`, the activations `A_j` at the current layer $j$ and $delta$ values for the next layer $j+1$, called `Delta_next`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the single layer network [1 pt]\n",
    "\n",
    "In case of a *Neural Network* with just 1 layer, this `weight_update` function will compute all the updated weights in a single step. Here `A_j` would refer to $A^0$, i.e. the activations at the input layer, and `Delta_next` would be the $\\delta$ terms for the output layer.\n",
    "\n",
    "Now if we just repeatedly compute the outputs, then the deltas, and then update the weights, the classification error on the training set should slowly decrease to a minimum. At that point it should be able to reproduce the training data with an as low as possible error.\n",
    "\n",
    "Write the function `one_layer_training`, which takes a dataset consisting of matrices `X` and `Y`, and a weight matrix `Theta_0` with some initial values. Additionally, the function takes 2 optional parameters; `iters` to indicate the number of iterations gradient descent should be repeated and `rate`, the learning rate that should be used for the updates. The function should then perform *Gradient Descent* on this dataset, and store the value of the cost function at every step. At the end the function should plot a graph showing the error decreasing and return the learned values for weight matrix `Theta_0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning boolean functions [1 pts]\n",
    "\n",
    "Lets now try and actually learn some boolean functions using our single layer network. Some boolean functions might be harder to learn than others, so it will be interesting to see what the network can and can't learn. Watch this video on the classic XOR problem.\n",
    "\n",
    "* [Intuitions 2](https://www.youtube.com/watch?v=QZqmNpEyiKI&index=49&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
    "\n",
    "Create training sets for 3 boolean functions: *AND*, *OR* and *XOR*, and train 1 layer networks for each of the these problems. Show the plots of the cost function decreasing and show the final produced outputs for each problem. Do these results match what you would expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cost_function(A, Y):\n",
    "    total = 0\n",
    "    M, K = A.shape\n",
    "    # Tired enough of vectors by now...\n",
    "    for i in range(M):\n",
    "        for j in range(K):\n",
    "            total -= Y[i, j] * np.log(A[i, j])\n",
    "            total -= (1 - Y[i, j]) * np.log(1 - A[i, j])\n",
    "    return total\n",
    "\n",
    "\n",
    "# def output_delta(A, Y):\n",
    "#     return A - Y\n",
    "\n",
    "\n",
    "# def weight_update(A_j, Delta_next, Theta_j, rate):\n",
    "\n",
    "#     dot = np.dot(np.transpose(Delta_next), A_j)\n",
    "#     deriv = np.asscalar(dot)\n",
    "\n",
    "#     #deriv = Delta_next\n",
    "#     return [Theta_j[0] - rate * deriv]\n",
    "\n",
    "\n",
    "# def one_layer_training(X, Y, Theta_0, iters=10000, rate=0.01):\n",
    "#     Xlist = []\n",
    "#     Ylist = []\n",
    "#     bX = add_bias(X)\n",
    "#     E = float('inf')\n",
    "#     for i in range(iters):\n",
    "#         A = n_layer_output(bX, Theta_0)\n",
    "#         newE = cost_function(A, Y)\n",
    "#         if newE > E:\n",
    "#             break\n",
    "#         E = newE\n",
    "#         Xlist.append(i)\n",
    "#         Ylist.append(E)\n",
    "\n",
    "#         Delta_next = A - Y\n",
    "#         Theta_0 = weight_update(A, Delta_next, Theta_0, rate)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.xlabel(\"Iterations\")\n",
    "#     plt.ylabel(\"Cost\")\n",
    "#     plt.plot(Xlist, Ylist)\n",
    "#     plt.show()\n",
    "#     print(Ylist[0], \"-->\", Ylist[-1])\n",
    "#     return Theta_0\n",
    "\n",
    "\n",
    "# AND = np.matrix([[1, 1],\n",
    "#                  [1, 0],\n",
    "#                  [0, 1],\n",
    "#                  [0, 0]])\n",
    "# bAND = add_bias(AND)\n",
    "# refAND = np.matrix([[1], [0], [0], [0]])\n",
    "\n",
    "# OR = np.matrix([[1, 1],\n",
    "#                 [1, 0],\n",
    "#                 [0, 1],\n",
    "#                 [0, 0]])\n",
    "# bOR = add_bias(OR)\n",
    "# refOR = np.matrix([[1], [1], [1], [0]])\n",
    "\n",
    "# print(\"OR:\")\n",
    "# init = n_layer_init(nbRows, 1)\n",
    "# print(init)\n",
    "# tOR = one_layer_training(OR, refOR, init)\n",
    "# print(tOR)\n",
    "# outOR = n_layer_output(bOR, tOR)\n",
    "# print(outOR)\n",
    "\n",
    "\n",
    "def plot(Xlist, Ylist):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.plot(Xlist, Ylist)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def delta(A, Y):\n",
    "    return A - Y\n",
    "\n",
    "\n",
    "def update(XT, delta):\n",
    "    return XT.dot(delta)\n",
    "\n",
    "\n",
    "def learn_one(X, Y, Theta, rate=0.9, iters=10000, thres=0.001):\n",
    "    E = float('inf')\n",
    "    Xlist = []\n",
    "    Ylist = []\n",
    "    XT = X.T\n",
    "    n_weights, n_layers = Theta.shape\n",
    "\n",
    "    # This mess can be used for selecting a particular\n",
    "    # layer: np.array([Theta[:, j]]).reshape(n_weights, 1)\n",
    "\n",
    "    for i in range(iters):\n",
    "        A = sigmoid_func(X.dot(Theta))\n",
    "        Theta -= rate * update(XT, delta(A, Y))\n",
    "\n",
    "        newE = cost_function(A, Y)\n",
    "        if newE > E - thres:\n",
    "            break\n",
    "        E = newE\n",
    "        Xlist.append(i)\n",
    "        Ylist.append(E)\n",
    "\n",
    "    plot(Xlist, Ylist)\n",
    "    return A, Theta\n",
    "\n",
    "\n",
    "X = add_bias(np.array([[1, 1],\n",
    "                       [1, 0],\n",
    "                       [0, 1],\n",
    "                       [0, 0]]))\n",
    "size_X = X.shape\n",
    "\n",
    "Y_or = np.array([[1],\n",
    "                 [1],\n",
    "                 [1],\n",
    "                 [0]])\n",
    "\n",
    "\n",
    "Y_and = np.array([[1],\n",
    "                  [0],\n",
    "                  [0],\n",
    "                  [0]])\n",
    "\n",
    "Y_xor = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])\n",
    "\n",
    "print(\"Or:\")\n",
    "Theta_0_or = np.array(n_layer_init(size_X[1], 1)[0])\n",
    "result = learn_one(X, Y_or, Theta_0_or)\n",
    "print(\"A:\\n\", result[0],\n",
    "      \"\\nWeights:\\n\", result[1])\n",
    "\n",
    "print(\"\\nAnd:\")\n",
    "Theta_0_and = np.array(n_layer_init(size_X[1], 1)[0])\n",
    "result = learn_one(X, Y_and, Theta_0_and)\n",
    "print(\"A:\\n\", result[0],\n",
    "      \"\\nWeights:\\n\", result[1])\n",
    "\n",
    "print(\"\\nXOR:\")\n",
    "Theta_0_xor = np.array(n_layer_init(size_X[1], 1)[0])\n",
    "result = learn_one(X, Y_xor, Theta_0_xor)\n",
    "print(\"A:\\n\", result[0],\n",
    "      \"\\nWeights:\\n\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation [1 pt]\n",
    "\n",
    "Until now we have only been training single layer networks, where the gradient was easy to compute using the error at the output nodes. The *Backpropagation* algorithm extends this by also computing error terms at the hidden nodes and thus being able to update multilayer networks in the direction of the gradient too.\n",
    "\n",
    "* [Backpropagation algorithm](https://www.youtube.com/watch?v=SvAEX5taVKk&index=52&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
    "\n",
    "Most of the functions you already wrote are still relevant for the complete *Backpropagtion*, the only real change is that there is now a $\\delta$ term for hidden nodes too.\n",
    "\n",
    "$$\\delta^j_i = (1 - A^j_i)A^j_i \\sum_k \\delta^{j+1}_k\\Theta^j_{ki} $$\n",
    "\n",
    "These $\\delta$'s take the error at the output layer and propagate it backwards, which is where the algorithm gets its name. The $\\delta$'s for the output layer and the gradient descent update of the weights remains exactly the same as for the single layer network.\n",
    "\n",
    "Write the function `hidden_delta`, which computes the matrix of $\\delta$ values for 1 hidden layer, where each row corresponding to the $\\delta$'s for a different training example and the columns correspond to each of the nodes in that hidden layer. It should take as input the activations at that layer $A^j$, the $delta$ values for the next layer and the matrix of weights connecting them $\\Theta^j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2 layers [2 pts]\n",
    "\n",
    "Now all that remains is to combine these functions into a multilayer network. These videos should help get an overview of the pieces you need:\n",
    "\n",
    "* [Backpropagation intuition](https://www.youtube.com/watch?v=q1bQDyV6lsg&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=53)\n",
    "* [Putting it all together](https://www.youtube.com/watch?v=T7-ZsYlFH4M&index=57&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
    "\n",
    "Write the function `two_layer_training`, which should work exactly like the function `one_layer_training`, except that it takes 2 weight matrix arguments $\\Theta^0$ and $\\Theta^1$. The argument $\\Theta^0$ should contain the weights connecting the input to the hidden layer and $\\Theta^1$ should contain the weights connecting the hidden layer to the output. The function should plot the cost function as it decreases with each iteration and return the computed values for $\\Theta^0$ and $\\Theta^1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting the boolean functions  [1 pt]\n",
    "\n",
    "Rerun the tests to learn the 3 boolean functions; *AND*, *OR* and *XOR*, but use the 2 layer network this time. Plot the cost function and show the final produced output for each.\n",
    "\n",
    "Briefly discuss if these results match your expectations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hidden_update(XT, A, Y):\n",
    "    return XT.dot(A - Y)\n",
    "    return (1 - A) * A * update(XT, delta(A, Y))\n",
    "\n",
    "\n",
    "def factor(output):\n",
    "    return (1 - output) * output\n",
    "\n",
    "\n",
    "def learn_two(X, Y, Theta_0, Theta_1,\n",
    "              iters=10000, rate=0.9, thres=1e-7, debug=False):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Theta_0 = np.array(Theta_0)\n",
    "    Theta_1 = np.array(Theta_1)\n",
    "\n",
    "    E = float('inf')\n",
    "    Xlist = []\n",
    "    Ylist = []\n",
    "    XT = X.T\n",
    "    for i in range(iters):\n",
    "        A_1 = sigmoid_func(X.dot(Theta_0))\n",
    "        A_2 = sigmoid_func(A_1.dot(Theta_1))\n",
    "        error2 = A_2 - Y\n",
    "        delta2 = factor(A_2) * error2\n",
    "        error1 = delta2 * Theta_1.T\n",
    "        delta1 = factor(A_1) * error1\n",
    "        if debug:\n",
    "            print(X.shape)\n",
    "            print(Theta_1.shape)\n",
    "            print(A_1.T.shape)\n",
    "            print(delta2.shape)\n",
    "        Theta_1 -= rate * (A_1.T.dot(delta2))\n",
    "        Theta_0 -= rate * (XT.dot(delta1))\n",
    "\n",
    "        newE = cost_function(A_2, Y)\n",
    "        if newE > E - thres:\n",
    "            break\n",
    "        E = newE\n",
    "        Xlist.append(i)\n",
    "        Ylist.append(E)\n",
    "\n",
    "    plot(Xlist, Ylist)\n",
    "    return Theta_0, Theta_1\n",
    "\n",
    "\n",
    "rand0 = n_layer_init(3, 4)[0]\n",
    "rand1 = n_layer_init(4, 1)[0]\n",
    "print(\"2-layer costs:\\nAND:\")\n",
    "result_and = learn_two(X, Y_and, rand0, rand1, thres=1e-5)\n",
    "print(\"OR:\")\n",
    "result_or = learn_two(X, Y_or, rand0, rand1, thres=1e-5)\n",
    "print(\"XOR:\")\n",
    "result_xor = learn_two(X, Y_xor, rand0, rand1, thres=1e-10)\n",
    "for i in result_xor:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit recognition\n",
    "\n",
    "For the last part of this assignment, lets try the algorithm on a little more complicated problem, digit recognition. The data for this problem can be found in the file `digits123.csv`. Each row contains 65 values, where the first 64 are greyscale pixel values, and the last value is the class label, corresponding to the digit being shown. The greyscale values are integers ranging from 1 to 16, and using some reshaping, can be reconstructed back into a crude *8x8* image. Below is the code to show the image for one of the digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your network [3 pts]\n",
    "\n",
    "Using this digits data and the neural network code you just wrote, try to train a network that can succesfully classify these images as the digit 1, 2 or 3. Tweak the parameters of the network until you get good prediction results and show the training and validation error for these cases. Briefly describe any tweaks you made to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLPClassifier [2 pts]\n",
    "\n",
    "Compare your results from the previous section with using an existing implementation from scikit-learn: [MLPClassifier](http://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification). Try and see if you can find a set of parameters for this *Neural Network* that boosts the score even further. The full list of parameter options can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier). Show the training and validation error for the best set of parameters you found and describe any additions you made to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = np.loadtxt('digits123.csv', delimiter=',', dtype=int)\n",
    "np.random.seed(5)\n",
    "np.random.shuffle(digits)\n",
    "nsamples, nfeatures = digits.shape\n",
    "ratio = 0.6\n",
    "cutrow = int(nsamples * ratio)\n",
    "training = (digits[:cutrow, :-1], digits[:cutrow, -1])\n",
    "validate = (digits[cutrow:, :-1], digits[cutrow:, -1])\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "digitC = MLPClassifier(solver='lbfgs', alpha=1e-7,\n",
    "                       hidden_layer_sizes=(20, 20), random_state=1)\n",
    "digitC.fit(training[0], training[1])\n",
    "result = digitC.predict(validate[0])\n",
    "cnt = 0\n",
    "for i in range(len(result)):\n",
    "    cnt += result[i] == validate[1][i]\n",
    "print(\"Scikit digit accuracy:\", cnt / len(result) * 100)\n",
    "\n",
    "Xlist = [[1., 1.],\n",
    "         [1., 0.],\n",
    "         [0., 1.],\n",
    "         [0., 0.]]\n",
    "\n",
    "xorC = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1)\n",
    "xorC.fit(Xlist, [0, 1, 1, 0])\n",
    "print(xorC.predict(Xlist))\n",
    "\n",
    "andC = MLPClassifier(solver='lbfgs', alpha=1 / float('inf'),\n",
    "                     hidden_layer_sizes=(1000,), random_state=1)\n",
    "andC.fit(Xlist, [1, 0, 0, 0])\n",
    "print(andC.predict(Xlist))\n",
    "\n",
    "print(\"\"\"\n",
    "Additions: freezing the seed at these values :).\n",
    "So yeah, it does do something and it can be not 100.\n",
    "\"\"\")\n",
    "\n",
    "print(\"This thing (digits with my NN) is TODO.\")\n",
    "l1 = n_layer_init(nfeatures, cutrow)[0]\n",
    "l2 = n_layer_init(cutrow, 1)[0]\n",
    "stupid_train_y = []\n",
    "for digit in training[1]:\n",
    "    stupid_train_y.append([digit])\n",
    "stupid_train_y = np.array(stupid_train_y).T\n",
    "network = learn_two(add_bias(training[0]), stupid_train_y,\n",
    "                    l1, l2, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
